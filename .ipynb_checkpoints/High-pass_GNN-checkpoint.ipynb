{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514bb662",
   "metadata": {},
   "source": [
    "# HP-GCN\n",
    "#### 分离图和独立节点的情况仅适用于数据集： YelpChi and amazon\n",
    "#### T-finance 和 tsocial 两个数据集对应的图中没有独立节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c23bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# 其中包括激活函数, 损失函数, 池化函数 ,通过 F.xxx() 的形式，可以方便地调用 torch.nn.functional 模块中的各种函数\n",
    "import numpy\n",
    "import argparse\n",
    "import time\n",
    "from dataset_process.dataset import Dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, roc_auc_score, precision_score, confusion_matrix\n",
    "from model.High_pass_anomaly_seperate import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9588b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, args):\n",
    "    features = g.ndata['feature']\n",
    "    labels = g.ndata['label']\n",
    "    index = list(range(len(labels)))\n",
    "    if dataset_name == 'amazon':\n",
    "        index = list(range(3305, len(labels)))\n",
    "\n",
    "    idx_train, idx_rest, y_train, y_rest = train_test_split(index, labels[index], stratify=labels[index],\n",
    "                                                            train_size=args.train_ratio,\n",
    "                                                            random_state=2, shuffle=True)\n",
    "    idx_valid, idx_test, y_valid, y_test = train_test_split(idx_rest, y_rest, stratify=y_rest,\n",
    "                                                            test_size=0.67,\n",
    "                                                            random_state=2, shuffle=True)\n",
    "    train_mask = torch.zeros([len(labels)]).bool()\n",
    "    val_mask = torch.zeros([len(labels)]).bool()\n",
    "    test_mask = torch.zeros([len(labels)]).bool()\n",
    "    print(\" y_train:\", y_train.shape, sum(y_train))\n",
    "    train_mask[idx_train] = 1\n",
    "    val_mask[idx_valid] = 1\n",
    "    test_mask[idx_test] = 1\n",
    "    print('train/dev/test samples: ', train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_f1, final_tf1, final_trec, final_tpre, final_tmf1, final_tauc = 0., 0., 0., 0., 0., 0.\n",
    "\n",
    "    weight = (1-labels[train_mask]).sum().item() / labels[train_mask].sum().item()\n",
    "    print('cross entropy weight: ', weight)\n",
    "    time_start = time.time()\n",
    "    for e in range(args.epoch):\n",
    "        # 训练\n",
    "        model.train()\n",
    "        # 调用模型中的forward函数\n",
    "        if args.homo:\n",
    "            logits = model(features,args.dataset)\n",
    "        else:\n",
    "            logits = model(features,args.dataset)\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask], weight=torch.tensor([1., weight]))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #验证\n",
    "        model.eval()\n",
    "        probs = logits.softmax(1)\n",
    "        f1, thres = get_best_f1(labels[val_mask], probs[val_mask])\n",
    "        preds = numpy.zeros_like(labels)\n",
    "        preds[probs[:, 1] > thres] = 1\n",
    "        trec = recall_score(labels[test_mask], preds[test_mask])\n",
    "        tpre = precision_score(labels[test_mask], preds[test_mask])\n",
    "        tmf1 = f1_score(labels[test_mask], preds[test_mask], average='macro')\n",
    "        tauc = roc_auc_score(labels[test_mask], probs[test_mask][:, 1].detach().numpy())\n",
    "\n",
    "        if best_f1 < f1:\n",
    "            best_f1 = f1\n",
    "            final_trec = trec\n",
    "            final_tpre = tpre\n",
    "            final_tmf1 = tmf1\n",
    "            final_tauc = tauc\n",
    "        print('Epoch {}, loss: {:.4f}, val mf1: {:.4f}, (best {:.4f})'.format(e, loss, f1, best_f1))\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('time cost: ', time_end - time_start, 's')\n",
    "    print('Test: REC {:.2f} PRE {:.2f} MF1 {:.2f} AUC {:.2f}'.format(final_trec*100,\n",
    "                                                                     final_tpre*100, final_tmf1*100, final_tauc*100))\n",
    "    return final_tmf1, final_tauc\n",
    "\n",
    "\n",
    "# threshold adjusting for best macro f1\n",
    "def get_best_f1(labels, probs):\n",
    "    best_f1, best_thre = 0, 0\n",
    "    for thres in np.linspace(0.05, 0.95, 19):\n",
    "        #构建一个与labels同维度的数组,并初始化所有变量为零\n",
    "        preds = np.zeros_like(labels)\n",
    "        preds[probs[:,1] > thres] = 1\n",
    "        #average='binary'：计算二分类问题中的 F1 分数（默认值）。\n",
    "        #average='micro'：对所有类别的真实和预测样本进行汇总，然后计算 F1 分数。\n",
    "        #average='macro'：计算每个类别的 F1 分数，然后取平均值。\n",
    "        #average=None：返回每个类别的 F1 分数。\n",
    "        # F1_score 详细原理间“备份”\n",
    "        mf1 = f1_score(labels, preds, average='macro')\n",
    "        if mf1 > best_f1:\n",
    "            best_f1 = mf1\n",
    "            best_thre = thres\n",
    "    return best_f1, best_thre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4be3b578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='yelp', train_ratio=0.4, hid_dim=64, homo=0, epoch=200, run=1, k=2)\n",
      "Done loading data from cached files.\n",
      "Graph(num_nodes={'review': 45954},\n",
      "      num_edges={('review', 'net_rsr', 'review'): 6805486, ('review', 'net_rtr', 'review'): 1147232, ('review', 'net_rur', 'review'): 98630},\n",
      "      metagraph=[('review', 'review', 'net_rsr'), ('review', 'review', 'net_rtr'), ('review', 'review', 'net_rur')])\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='HP_GCN')\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"yelp\",\n",
    "                        help=\"Dataset for this model (yelp/amazon/tfinance/tsocial)\")\n",
    "parser.add_argument(\"--train_ratio\", type=float, default=0.4, help=\"Training ratio\")\n",
    "parser.add_argument(\"--hid_dim\", type=int, default=64, help=\"Hidden layer dimension\")\n",
    "parser.add_argument(\"--homo\", type=int, default=0, help=\"1 for HP_GCN(Homo) and 0 for HP_GCN(Hetero)\")\n",
    "#For yelp：epoch = 200\n",
    "#For amazon/tfinance/tsocial： epoch = 100\n",
    "parser.add_argument(\"--epoch\", type=int, default=200, help=\"The max number of epochs\")\n",
    "parser.add_argument(\"--run\", type=int, default=1, help=\"Running times\")\n",
    "parser.add_argument(\"--k\", type=int, default=2, help=\"k in ChebConv\")\n",
    "\n",
    "\n",
    "args = parser.parse_args(args = [])\n",
    "print(args)\n",
    "dataset_name = args.dataset\n",
    "homo = args.homo\n",
    "k = args.k\n",
    "h_feats = args.hid_dim\n",
    "graph = Dataset(dataset_name, homo).graph\n",
    "#edge_index = Dataset(dataset_name, homo).edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c23b15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " y_train: torch.Size([18381]) tensor(2671)\n",
      "train/dev/test samples:  18381 9099 18474\n",
      "cross entropy weight:  5.8816922500935975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.6932, val mf1: 0.4608, (best 0.4608)\n",
      "Epoch 1, loss: 0.6408, val mf1: 0.6063, (best 0.6063)\n",
      "Epoch 2, loss: 0.5926, val mf1: 0.5837, (best 0.6063)\n",
      "Epoch 3, loss: 0.6855, val mf1: 0.5790, (best 0.6063)\n",
      "Epoch 4, loss: 0.5672, val mf1: 0.5790, (best 0.6063)\n",
      "Epoch 5, loss: 0.5864, val mf1: 0.5790, (best 0.6063)\n",
      "Epoch 6, loss: 0.5928, val mf1: 0.5790, (best 0.6063)\n",
      "Epoch 7, loss: 0.5863, val mf1: 0.5790, (best 0.6063)\n",
      "Epoch 8, loss: 0.5756, val mf1: 0.5790, (best 0.6063)\n",
      "Epoch 9, loss: 0.5689, val mf1: 0.5790, (best 0.6063)\n",
      "Epoch 10, loss: 0.5590, val mf1: 0.5793, (best 0.6063)\n",
      "Epoch 11, loss: 0.5661, val mf1: 0.5795, (best 0.6063)\n",
      "Epoch 12, loss: 0.5567, val mf1: 0.5853, (best 0.6063)\n",
      "Epoch 13, loss: 0.5571, val mf1: 0.6132, (best 0.6132)\n",
      "Epoch 14, loss: 0.5552, val mf1: 0.5795, (best 0.6132)\n",
      "Epoch 15, loss: 0.5557, val mf1: 0.6048, (best 0.6132)\n",
      "Epoch 16, loss: 0.5506, val mf1: 0.6206, (best 0.6206)\n",
      "Epoch 17, loss: 0.5475, val mf1: 0.6097, (best 0.6206)\n",
      "Epoch 18, loss: 0.5425, val mf1: 0.6465, (best 0.6465)\n",
      "Epoch 19, loss: 0.5359, val mf1: 0.6771, (best 0.6771)\n",
      "Epoch 20, loss: 0.5260, val mf1: 0.7290, (best 0.7290)\n",
      "Epoch 21, loss: 0.5153, val mf1: 0.7380, (best 0.7380)\n",
      "Epoch 22, loss: 0.5088, val mf1: 0.7364, (best 0.7380)\n",
      "Epoch 23, loss: 0.5424, val mf1: 0.7445, (best 0.7445)\n",
      "Epoch 24, loss: 0.5412, val mf1: 0.7301, (best 0.7445)\n",
      "Epoch 25, loss: 0.5554, val mf1: 0.7295, (best 0.7445)\n",
      "Epoch 26, loss: 0.4820, val mf1: 0.7434, (best 0.7445)\n",
      "Epoch 27, loss: 0.5150, val mf1: 0.7488, (best 0.7488)\n",
      "Epoch 28, loss: 0.4914, val mf1: 0.7526, (best 0.7526)\n",
      "Epoch 29, loss: 0.4656, val mf1: 0.7579, (best 0.7579)\n",
      "Epoch 30, loss: 0.4558, val mf1: 0.7679, (best 0.7679)\n",
      "Epoch 31, loss: 0.4288, val mf1: 0.7885, (best 0.7885)\n",
      "Epoch 32, loss: 0.4220, val mf1: 0.7951, (best 0.7951)\n",
      "Epoch 33, loss: 0.4521, val mf1: 0.7972, (best 0.7972)\n",
      "Epoch 34, loss: 0.5907, val mf1: 0.7440, (best 0.7972)\n",
      "Epoch 35, loss: 0.6072, val mf1: 0.7198, (best 0.7972)\n",
      "Epoch 36, loss: 0.5674, val mf1: 0.6921, (best 0.7972)\n",
      "Epoch 37, loss: 0.5377, val mf1: 0.6874, (best 0.7972)\n",
      "Epoch 38, loss: 0.5114, val mf1: 0.7046, (best 0.7972)\n",
      "Epoch 39, loss: 0.4607, val mf1: 0.7441, (best 0.7972)\n",
      "Epoch 40, loss: 0.4249, val mf1: 0.7742, (best 0.7972)\n",
      "Epoch 41, loss: 0.4500, val mf1: 0.7776, (best 0.7972)\n",
      "Epoch 42, loss: 0.4698, val mf1: 0.7693, (best 0.7972)\n",
      "Epoch 43, loss: 0.4575, val mf1: 0.7851, (best 0.7972)\n",
      "Epoch 44, loss: 0.3952, val mf1: 0.7813, (best 0.7972)\n",
      "Epoch 45, loss: 0.4012, val mf1: 0.7794, (best 0.7972)\n",
      "Epoch 46, loss: 0.3977, val mf1: 0.7902, (best 0.7972)\n",
      "Epoch 47, loss: 0.3756, val mf1: 0.8008, (best 0.8008)\n",
      "Epoch 48, loss: 0.3636, val mf1: 0.8124, (best 0.8124)\n",
      "Epoch 49, loss: 0.3615, val mf1: 0.8098, (best 0.8124)\n",
      "Epoch 50, loss: 0.3430, val mf1: 0.8162, (best 0.8162)\n",
      "Epoch 51, loss: 0.3470, val mf1: 0.8194, (best 0.8194)\n",
      "Epoch 52, loss: 0.3401, val mf1: 0.8260, (best 0.8260)\n",
      "Epoch 53, loss: 0.3294, val mf1: 0.8291, (best 0.8291)\n",
      "Epoch 54, loss: 0.3301, val mf1: 0.8285, (best 0.8291)\n",
      "Epoch 55, loss: 0.3232, val mf1: 0.8370, (best 0.8370)\n",
      "Epoch 56, loss: 0.3388, val mf1: 0.8352, (best 0.8370)\n",
      "Epoch 57, loss: 0.3654, val mf1: 0.8324, (best 0.8370)\n",
      "Epoch 58, loss: 0.3854, val mf1: 0.8312, (best 0.8370)\n",
      "Epoch 59, loss: 0.3283, val mf1: 0.8377, (best 0.8377)\n",
      "Epoch 60, loss: 0.3557, val mf1: 0.8432, (best 0.8432)\n",
      "Epoch 61, loss: 0.3208, val mf1: 0.8411, (best 0.8432)\n",
      "Epoch 62, loss: 0.3371, val mf1: 0.8323, (best 0.8432)\n",
      "Epoch 63, loss: 0.3047, val mf1: 0.8369, (best 0.8432)\n",
      "Epoch 64, loss: 0.3094, val mf1: 0.8423, (best 0.8432)\n",
      "Epoch 65, loss: 0.3024, val mf1: 0.8375, (best 0.8432)\n",
      "Epoch 66, loss: 0.2944, val mf1: 0.8368, (best 0.8432)\n",
      "Epoch 67, loss: 0.2937, val mf1: 0.8444, (best 0.8444)\n",
      "Epoch 68, loss: 0.2832, val mf1: 0.8498, (best 0.8498)\n",
      "Epoch 69, loss: 0.2799, val mf1: 0.8488, (best 0.8498)\n",
      "Epoch 70, loss: 0.2711, val mf1: 0.8515, (best 0.8515)\n",
      "Epoch 71, loss: 0.2714, val mf1: 0.8549, (best 0.8549)\n",
      "Epoch 72, loss: 0.2992, val mf1: 0.8513, (best 0.8549)\n",
      "Epoch 73, loss: 0.5002, val mf1: 0.8511, (best 0.8549)\n",
      "Epoch 74, loss: 0.4370, val mf1: 0.8361, (best 0.8549)\n",
      "Epoch 75, loss: 0.4132, val mf1: 0.8437, (best 0.8549)\n",
      "Epoch 76, loss: 0.3587, val mf1: 0.8438, (best 0.8549)\n",
      "Epoch 77, loss: 0.3672, val mf1: 0.8326, (best 0.8549)\n",
      "Epoch 78, loss: 0.3780, val mf1: 0.8288, (best 0.8549)\n",
      "Epoch 79, loss: 0.3785, val mf1: 0.8414, (best 0.8549)\n",
      "Epoch 80, loss: 0.3515, val mf1: 0.8518, (best 0.8549)\n",
      "Epoch 81, loss: 0.3118, val mf1: 0.8548, (best 0.8549)\n",
      "Epoch 82, loss: 0.2968, val mf1: 0.8476, (best 0.8549)\n",
      "Epoch 83, loss: 0.3006, val mf1: 0.8391, (best 0.8549)\n",
      "Epoch 84, loss: 0.3034, val mf1: 0.8341, (best 0.8549)\n",
      "Epoch 85, loss: 0.3074, val mf1: 0.8352, (best 0.8549)\n",
      "Epoch 86, loss: 0.2907, val mf1: 0.8440, (best 0.8549)\n",
      "Epoch 87, loss: 0.2799, val mf1: 0.8526, (best 0.8549)\n",
      "Epoch 88, loss: 0.2699, val mf1: 0.8594, (best 0.8594)\n",
      "Epoch 89, loss: 0.2694, val mf1: 0.8598, (best 0.8598)\n",
      "Epoch 90, loss: 0.2703, val mf1: 0.8604, (best 0.8604)\n",
      "Epoch 91, loss: 0.2660, val mf1: 0.8597, (best 0.8604)\n",
      "Epoch 92, loss: 0.2603, val mf1: 0.8592, (best 0.8604)\n",
      "Epoch 93, loss: 0.2529, val mf1: 0.8636, (best 0.8636)\n",
      "Epoch 94, loss: 0.2487, val mf1: 0.8615, (best 0.8636)\n",
      "Epoch 95, loss: 0.2462, val mf1: 0.8652, (best 0.8652)\n",
      "Epoch 96, loss: 0.2427, val mf1: 0.8641, (best 0.8652)\n",
      "Epoch 97, loss: 0.2396, val mf1: 0.8670, (best 0.8670)\n",
      "Epoch 98, loss: 0.2342, val mf1: 0.8684, (best 0.8684)\n",
      "Epoch 99, loss: 0.2319, val mf1: 0.8715, (best 0.8715)\n",
      "Epoch 100, loss: 0.2294, val mf1: 0.8751, (best 0.8751)\n",
      "Epoch 101, loss: 0.2256, val mf1: 0.8805, (best 0.8805)\n",
      "Epoch 102, loss: 0.2224, val mf1: 0.8763, (best 0.8805)\n",
      "Epoch 103, loss: 0.2190, val mf1: 0.8779, (best 0.8805)\n",
      "Epoch 104, loss: 0.2168, val mf1: 0.8785, (best 0.8805)\n",
      "Epoch 105, loss: 0.2171, val mf1: 0.8815, (best 0.8815)\n",
      "Epoch 106, loss: 0.2242, val mf1: 0.8777, (best 0.8815)\n",
      "Epoch 107, loss: 0.2679, val mf1: 0.8817, (best 0.8817)\n",
      "Epoch 108, loss: 0.3705, val mf1: 0.8649, (best 0.8817)\n",
      "Epoch 109, loss: 0.2864, val mf1: 0.8669, (best 0.8817)\n",
      "Epoch 110, loss: 0.3742, val mf1: 0.8667, (best 0.8817)\n",
      "Epoch 111, loss: 0.2898, val mf1: 0.8633, (best 0.8817)\n",
      "Epoch 112, loss: 0.3115, val mf1: 0.8587, (best 0.8817)\n",
      "Epoch 113, loss: 0.3170, val mf1: 0.8570, (best 0.8817)\n",
      "Epoch 114, loss: 0.2722, val mf1: 0.8600, (best 0.8817)\n",
      "Epoch 115, loss: 0.2739, val mf1: 0.8617, (best 0.8817)\n",
      "Epoch 116, loss: 0.2627, val mf1: 0.8643, (best 0.8817)\n",
      "Epoch 117, loss: 0.2451, val mf1: 0.8644, (best 0.8817)\n",
      "Epoch 118, loss: 0.2532, val mf1: 0.8669, (best 0.8817)\n",
      "Epoch 119, loss: 0.2462, val mf1: 0.8707, (best 0.8817)\n",
      "Epoch 120, loss: 0.2354, val mf1: 0.8757, (best 0.8817)\n",
      "Epoch 121, loss: 0.2344, val mf1: 0.8750, (best 0.8817)\n",
      "Epoch 122, loss: 0.2322, val mf1: 0.8714, (best 0.8817)\n",
      "Epoch 123, loss: 0.2263, val mf1: 0.8761, (best 0.8817)\n",
      "Epoch 124, loss: 0.2267, val mf1: 0.8783, (best 0.8817)\n",
      "Epoch 125, loss: 0.2193, val mf1: 0.8829, (best 0.8829)\n",
      "Epoch 126, loss: 0.2200, val mf1: 0.8841, (best 0.8841)\n",
      "Epoch 127, loss: 0.2156, val mf1: 0.8840, (best 0.8841)\n",
      "Epoch 128, loss: 0.2134, val mf1: 0.8854, (best 0.8854)\n",
      "Epoch 129, loss: 0.2123, val mf1: 0.8870, (best 0.8870)\n",
      "Epoch 130, loss: 0.2116, val mf1: 0.8869, (best 0.8870)\n",
      "Epoch 131, loss: 0.2083, val mf1: 0.8894, (best 0.8894)\n",
      "Epoch 132, loss: 0.2058, val mf1: 0.8889, (best 0.8894)\n",
      "Epoch 133, loss: 0.2035, val mf1: 0.8928, (best 0.8928)\n",
      "Epoch 134, loss: 0.2027, val mf1: 0.8916, (best 0.8928)\n",
      "Epoch 135, loss: 0.1996, val mf1: 0.8921, (best 0.8928)\n",
      "Epoch 136, loss: 0.1992, val mf1: 0.8929, (best 0.8929)\n",
      "Epoch 137, loss: 0.1947, val mf1: 0.8950, (best 0.8950)\n",
      "Epoch 138, loss: 0.1933, val mf1: 0.8957, (best 0.8957)\n",
      "Epoch 139, loss: 0.1929, val mf1: 0.8949, (best 0.8957)\n",
      "Epoch 140, loss: 0.1895, val mf1: 0.8982, (best 0.8982)\n",
      "Epoch 141, loss: 0.1869, val mf1: 0.8987, (best 0.8987)\n",
      "Epoch 142, loss: 0.1855, val mf1: 0.9012, (best 0.9012)\n",
      "Epoch 143, loss: 0.1847, val mf1: 0.9021, (best 0.9021)\n",
      "Epoch 144, loss: 0.1822, val mf1: 0.9028, (best 0.9028)\n",
      "Epoch 145, loss: 0.1800, val mf1: 0.9042, (best 0.9042)\n",
      "Epoch 146, loss: 0.1784, val mf1: 0.9031, (best 0.9042)\n",
      "Epoch 147, loss: 0.1772, val mf1: 0.9031, (best 0.9042)\n",
      "Epoch 148, loss: 0.1755, val mf1: 0.9054, (best 0.9054)\n",
      "Epoch 149, loss: 0.1740, val mf1: 0.9051, (best 0.9054)\n",
      "Epoch 150, loss: 0.1728, val mf1: 0.9078, (best 0.9078)\n",
      "Epoch 151, loss: 0.1724, val mf1: 0.9073, (best 0.9078)\n",
      "Epoch 152, loss: 0.1752, val mf1: 0.9093, (best 0.9093)\n",
      "Epoch 153, loss: 0.1895, val mf1: 0.9067, (best 0.9093)\n",
      "Epoch 154, loss: 0.1921, val mf1: 0.9030, (best 0.9093)\n",
      "Epoch 155, loss: 0.1808, val mf1: 0.9071, (best 0.9093)\n",
      "Epoch 156, loss: 0.1713, val mf1: 0.9101, (best 0.9101)\n",
      "Epoch 157, loss: 0.1854, val mf1: 0.9092, (best 0.9101)\n",
      "Epoch 158, loss: 0.1704, val mf1: 0.9099, (best 0.9101)\n",
      "Epoch 159, loss: 0.1748, val mf1: 0.9081, (best 0.9101)\n",
      "Epoch 160, loss: 0.1676, val mf1: 0.9099, (best 0.9101)\n",
      "Epoch 161, loss: 0.1687, val mf1: 0.9118, (best 0.9118)\n",
      "Epoch 162, loss: 0.1672, val mf1: 0.9104, (best 0.9118)\n",
      "Epoch 163, loss: 0.1636, val mf1: 0.9103, (best 0.9118)\n",
      "Epoch 164, loss: 0.1654, val mf1: 0.9120, (best 0.9120)\n",
      "Epoch 165, loss: 0.1602, val mf1: 0.9137, (best 0.9137)\n",
      "Epoch 166, loss: 0.1623, val mf1: 0.9138, (best 0.9138)\n",
      "Epoch 167, loss: 0.1571, val mf1: 0.9135, (best 0.9138)\n",
      "Epoch 168, loss: 0.1596, val mf1: 0.9140, (best 0.9140)\n",
      "Epoch 169, loss: 0.1561, val mf1: 0.9152, (best 0.9152)\n",
      "Epoch 170, loss: 0.1568, val mf1: 0.9119, (best 0.9152)\n",
      "Epoch 171, loss: 0.1528, val mf1: 0.9154, (best 0.9154)\n",
      "Epoch 172, loss: 0.1537, val mf1: 0.9182, (best 0.9182)\n",
      "Epoch 173, loss: 0.1518, val mf1: 0.9146, (best 0.9182)\n",
      "Epoch 174, loss: 0.1503, val mf1: 0.9145, (best 0.9182)\n",
      "Epoch 175, loss: 0.1496, val mf1: 0.9185, (best 0.9185)\n",
      "Epoch 176, loss: 0.1471, val mf1: 0.9183, (best 0.9185)\n",
      "Epoch 177, loss: 0.1479, val mf1: 0.9150, (best 0.9185)\n",
      "Epoch 178, loss: 0.1449, val mf1: 0.9170, (best 0.9185)\n",
      "Epoch 179, loss: 0.1449, val mf1: 0.9201, (best 0.9201)\n",
      "Epoch 180, loss: 0.1430, val mf1: 0.9189, (best 0.9201)\n",
      "Epoch 181, loss: 0.1425, val mf1: 0.9168, (best 0.9201)\n",
      "Epoch 182, loss: 0.1411, val mf1: 0.9181, (best 0.9201)\n",
      "Epoch 183, loss: 0.1406, val mf1: 0.9203, (best 0.9203)\n",
      "Epoch 184, loss: 0.1416, val mf1: 0.9172, (best 0.9203)\n",
      "Epoch 185, loss: 0.1452, val mf1: 0.9172, (best 0.9203)\n",
      "Epoch 186, loss: 0.1643, val mf1: 0.9113, (best 0.9203)\n",
      "Epoch 187, loss: 0.1875, val mf1: 0.9083, (best 0.9203)\n",
      "Epoch 188, loss: 0.1774, val mf1: 0.9142, (best 0.9203)\n",
      "Epoch 189, loss: 0.1613, val mf1: 0.9105, (best 0.9203)\n",
      "Epoch 190, loss: 0.1653, val mf1: 0.9191, (best 0.9203)\n",
      "Epoch 191, loss: 0.1520, val mf1: 0.9138, (best 0.9203)\n",
      "Epoch 192, loss: 0.1632, val mf1: 0.9078, (best 0.9203)\n",
      "Epoch 193, loss: 0.1491, val mf1: 0.9151, (best 0.9203)\n",
      "Epoch 194, loss: 0.1562, val mf1: 0.9173, (best 0.9203)\n",
      "Epoch 195, loss: 0.1459, val mf1: 0.9179, (best 0.9203)\n",
      "Epoch 196, loss: 0.1620, val mf1: 0.9133, (best 0.9203)\n",
      "Epoch 197, loss: 0.1496, val mf1: 0.9161, (best 0.9203)\n",
      "Epoch 198, loss: 0.1472, val mf1: 0.9201, (best 0.9203)\n",
      "Epoch 199, loss: 0.1485, val mf1: 0.9193, (best 0.9203)\n",
      "time cost:  1658.6216146945953 s\n",
      "Test: REC 86.89 PRE 85.80 MF1 92.00 AUC 98.30\n",
      "MF1-mean: 92.00, MF1-std: 0.00, AUC-mean: 98.30, AUC-std: 0.00\n"
     ]
    }
   ],
   "source": [
    "in_feats = graph.ndata['feature'].shape[1]\n",
    "num_classes = 2\n",
    "\n",
    "if args.run == 0:\n",
    "    if homo:\n",
    "        print(\"hello\")\n",
    "        model = ChebConvGAD(in_feats, h_feats, num_classes, graph,k = k)\n",
    "    else:\n",
    "        model = ChebConvGAD_Hetero(in_feats, h_feats, num_classes,k = k)\n",
    "    train(model, graph, args)\n",
    "\n",
    "else:\n",
    "    final_mf1s, final_aucs = [], []\n",
    "    for tt in range(args.run):\n",
    "        if homo:\n",
    "            #in_feats 特征点维度；h_feats：隐层维度；num_classes：节点分类数（nomal，anomaly）\n",
    "            model = ChebConvGAD(in_feats, h_feats, num_classes, graph,k = k)\n",
    "        else:\n",
    "            model = ChebConvGAD_Hetero(in_feats, h_feats, num_classes, graph, k = k)\n",
    "        mf1, auc = train(model, graph, args)\n",
    "        final_mf1s.append(mf1)\n",
    "        final_aucs.append(auc)\n",
    "    final_mf1s = np.array(final_mf1s)\n",
    "    final_aucs = np.array(final_aucs)\n",
    "    # np.std :计算全局标准差\n",
    "    print('MF1-mean: {:.2f}, MF1-std: {:.2f}, AUC-mean: {:.2f}, AUC-std: {:.2f}'.format(100 * np.mean(final_mf1s),\n",
    "                                                                                            100 * np.std(final_mf1s),\n",
    "                                                               100 * np.mean(final_aucs), 100 * np.std(final_aucs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04f201-da48-4ac2-8633-708d73f78082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
