{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7372750",
   "metadata": {},
   "source": [
    "# model source \n",
    "### Training CARE-GNN\n",
    "### Paper: Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters\n",
    "### Source: https://github.com/YingtongDou/CARE-GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bf56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# 其中包括激活函数, 损失函数, 池化函数 ,通过 F.xxx() 的形式，可以方便地调用 torch.nn.functional 模块中的各种函数\n",
    "import numpy as np\n",
    "import numpy\n",
    "import argparse\n",
    "import time\n",
    "from dataset_process.dataset import Dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, roc_auc_score, precision_score, confusion_matrix\n",
    "from model.CAREGNN_anomaly import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from layers.CAREGNN_layers.CAREGNN_layers_hetero import *\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95601549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, args):\n",
    "    features = g.ndata['feature']\n",
    "    labels = g.ndata['label']\n",
    "    index = list(range(len(labels)))\n",
    "    if dataset_name == 'amazon':\n",
    "        index = list(range(3305, len(labels)))\n",
    "\n",
    "    idx_train, idx_rest, y_train, y_rest = train_test_split(index, labels[index], stratify=labels[index],\n",
    "                                                            train_size=args.train_ratio,\n",
    "                                                            random_state=2, shuffle=True)\n",
    "    idx_valid, idx_test, y_valid, y_test = train_test_split(idx_rest, y_rest, stratify=y_rest,\n",
    "                                                            test_size=0.67,\n",
    "                                                            random_state=2, shuffle=True)\n",
    "    train_mask = torch.zeros([len(labels)]).bool()\n",
    "    val_mask = torch.zeros([len(labels)]).bool()\n",
    "    test_mask = torch.zeros([len(labels)]).bool()\n",
    "\n",
    "    train_mask[idx_train] = 1\n",
    "    val_mask[idx_valid] = 1\n",
    "    test_mask[idx_test] = 1\n",
    "    print('train/dev/test samples: ', train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_f1, final_tf1, final_trec, final_tpre, final_tmf1, final_tauc = 0., 0., 0., 0., 0., 0.\n",
    "\n",
    "    weight = (1-labels[train_mask]).sum().item() / labels[train_mask].sum().item()\n",
    "    print('cross entropy weight: ', weight)\n",
    "    time_start = time.time()\n",
    "    for e in range(args.epoch):\n",
    "        # 训练\n",
    "        model.train()\n",
    "        # 调用模型中的forward函数\n",
    "        logits = model(features)\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask], weight=torch.tensor([1., weight]))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #验证\n",
    "        model.eval()\n",
    "        probs = logits.softmax(1)\n",
    "        f1, thres = get_best_f1(labels[val_mask], probs[val_mask])\n",
    "        preds = np.zeros_like(labels)\n",
    "        preds[probs[:, 1] > thres] = 1\n",
    "        trec = recall_score(labels[test_mask], preds[test_mask])\n",
    "        tpre = precision_score(labels[test_mask], preds[test_mask])\n",
    "        tmf1 = f1_score(labels[test_mask], preds[test_mask], average='macro')\n",
    "        tauc = roc_auc_score(labels[test_mask], probs[test_mask][:, 1].detach().numpy())\n",
    "\n",
    "        if best_f1 < f1:\n",
    "            best_f1 = f1\n",
    "            final_trec = trec\n",
    "            final_tpre = tpre\n",
    "            final_tmf1 = tmf1\n",
    "            final_tauc = tauc\n",
    "        print('Epoch {}, loss: {:.4f}, val mf1: {:.4f}, (best {:.4f})'.format(e, loss, f1, best_f1))\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('time cost: ', time_end - time_start, 's')\n",
    "    print('Test: REC {:.2f} PRE {:.2f} MF1 {:.2f} AUC {:.2f}'.format(final_trec*100,\n",
    "                                                                     final_tpre*100, final_tmf1*100, final_tauc*100))\n",
    "    return final_tmf1, final_tauc\n",
    "\n",
    "\n",
    "# threshold adjusting for best macro f1\n",
    "def get_best_f1(labels, probs):\n",
    "    best_f1, best_thre = 0, 0\n",
    "    for thres in np.linspace(0.05, 0.95, 19):\n",
    "        #构建一个与labels同维度的数组,并初始化所有变量为零\n",
    "        preds = np.zeros_like(labels)\n",
    "        preds[probs[:,1] > thres] = 1\n",
    "        #average='binary'：计算二分类问题中的 F1 分数（默认值）。\n",
    "        #average='micro'：对所有类别的真实和预测样本进行汇总，然后计算 F1 分数。\n",
    "        #average='macro'：计算每个类别的 F1 分数，然后取平均值。\n",
    "        #average=None：返回每个类别的 F1 分数。\n",
    "        # F1_score 详细原理间“备份”\n",
    "        mf1 = f1_score(labels, preds, average='macro')\n",
    "        if mf1 > best_f1:\n",
    "            best_f1 = mf1\n",
    "            best_thre = thres\n",
    "    return best_f1, best_thre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cdf3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='CAREGNN_GAD')\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"yelp\",\n",
    "                        help=\"Dataset for this model (yelp/amazon/tfinance/tsocial)\")\n",
    "parser.add_argument(\"--train_ratio\", type=float, default=0.01, help=\"Training ratio\")\n",
    "parser.add_argument(\"--hid_dim\", type=int, default=64, help=\"Hidden layer dimension\")\n",
    "parser.add_argument(\"--homo\", type=int, default= 0, help=\"1 for CAREGNN_GAD(Homo) and 0 for CAREGNN_GAD(Hetero)\")\n",
    "parser.add_argument(\"--epoch\", type=int, default=100, help=\"The max number of epochs\")\n",
    "parser.add_argument(\"--run\", type=int, default=1, help=\"Running times\")\n",
    "parser.add_argument('--lambda_1', type=float, default=2, help='Simi loss weight.')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "parser.add_argument('--emb-size', type=int, default=64, help='Node embedding size at the last layer.')\n",
    "parser.add_argument('--inter', type=str, default='GNN', help='The inter-relation aggregator type. [Att, Weight, Mean, GNN]')\n",
    "parser.add_argument('--step-size', type=float, default=2e-2, help='RL action step size')\n",
    "parser.add_argument('--model', type=str, default='CARE', help='The model name. [CARE, SAGE]')\n",
    "parser.add_argument('--batch-size', type=int, default=1024, help='Batch size 1024 for yelp, 256 for amazon.')\n",
    "\n",
    "args = parser.parse_args(args = [])\n",
    "#args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "args.cuda = False\n",
    "\n",
    "print(args)\n",
    "dataset_name = args.dataset\n",
    "homo = args.homo\n",
    "h_feats = args.hid_dim\n",
    "graph = Dataset(dataset_name, homo).graph\n",
    "#edge_index = Dataset(dataset_name, homo).edge_index\n",
    "\n",
    "if (homo):\n",
    "    from layers.CAREGNN_layers.CAREGNN_layers_homo import *\n",
    "else:\n",
    "    from layers.CAREGNN_layers.CAREGNN_layers_hetero import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "############        从 DGL 图中获取图节点的邻居\n",
    "##################################################################################\n",
    "if (homo):\n",
    "    adj_lists = defaultdict(set)\n",
    "    # 获取每个节点的邻居节点并存储为 frozenset\n",
    "    for node in range(graph.num_nodes()):\n",
    "        neighbors = graph.successors(node)  # 对于出边邻居，使用 successors\n",
    "        for value in neighbors.tolist():\n",
    "            adj_lists[node].add(value)  \n",
    "else: ## only (dataset_name =='yelp') || (dataset_name =='yelp')\n",
    "    adj_list0 = defaultdict(set)\n",
    "    # 获取每个节点的邻居节点并存储为 frozenset\n",
    "    for node in range(graph[graph.canonical_etypes[0]].num_nodes()):\n",
    "        neighbors = graph[graph.canonical_etypes[0]].successors(node)  # 对于出边邻居，使用 successors\n",
    "        for value in neighbors.tolist():\n",
    "            adj_list0[node].add(value)\n",
    "                \n",
    "    adj_list1 = defaultdict(set)\n",
    "    # 获取每个节点的邻居节点并存储为 frozenset\n",
    "    for node in range(graph[graph.canonical_etypes[1]].num_nodes()):\n",
    "        neighbors = graph[graph.canonical_etypes[1]].successors(node)  # 对于出边邻居，使用 successors\n",
    "        for value in neighbors.tolist():\n",
    "            adj_list1[node].add(value)\n",
    "        \n",
    "    adj_list2 = defaultdict(set)\n",
    "    # 获取每个节点的邻居节点并存储为 frozenset\n",
    "    for node in range(graph[graph.canonical_etypes[2]].num_nodes()):\n",
    "        neighbors = graph[graph.canonical_etypes[2]].successors(node)  # 对于出边邻居，使用 successors\n",
    "        for value in neighbors.tolist():\n",
    "            adj_list2[node].add(value)\n",
    "    \n",
    "    adj_lists = [adj_list0, adj_list1, adj_list2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feats = graph.ndata['feature'].shape[1]\n",
    "features = graph.ndata['feature']\n",
    "num_classes = 2\n",
    "\n",
    "lambda_1 = args.lambda_1\n",
    "\n",
    "if (homo):\n",
    "    # build one-layer models\n",
    "    intra1 = IntraAgg(features, in_feats, cuda=args.cuda)\n",
    "    inter1 = InterAgg(features, in_feats, args.emb_size, adj_lists, [intra1], inter=args.inter,\n",
    "                  step_size=args.step_size, cuda=args.cuda)\n",
    "else:\n",
    "    # build one-layer models\n",
    "    intra1 = IntraAgg(features, in_feats, cuda=args.cuda)\n",
    "    intra2 = IntraAgg(features, in_feats, cuda=args.cuda)\n",
    "    intra3 = IntraAgg(features, in_feats, cuda=args.cuda)\n",
    "    inter1 = InterAgg(features, in_feats, args.emb_size, adj_lists, [intra1, intra2, intra3], inter=args.inter,\n",
    "                  step_size=args.step_size, cuda=args.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21660731",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.run == 0:\n",
    "    \n",
    "    model = CAREGNN_GAD(in_feats, h_feats, num_classes, graph,lambda_1)\n",
    "    train(model, graph, args)\n",
    "\n",
    "else:\n",
    "    \n",
    "    final_mf1s, final_aucs = [], []\n",
    "    for tt in range(args.run):\n",
    "        #in_feats 特征点维度；h_feats：隐层维度；num_classes：节点分类数（nomal，anomaly）\n",
    "        model = CAREGNN_GAD(in_feats, h_feats, num_classes, graph,inter1,lambda_1)\n",
    "        \n",
    "        mf1, auc = train(model, graph, args)\n",
    "        final_mf1s.append(mf1)\n",
    "        final_aucs.append(auc)\n",
    "    final_mf1s = np.array(final_mf1s)\n",
    "    final_aucs = np.array(final_aucs)\n",
    "    # np.std :计算全局标准差\n",
    "    print('MF1-mean: {:.2f}, MF1-std: {:.2f}, AUC-mean: {:.2f}, AUC-std: {:.2f}'.format(100 * np.mean(final_mf1s),\n",
    "                                                                                            100 * np.std(final_mf1s),\n",
    "                                                               100 * np.mean(final_aucs), 100 * np.std(final_aucs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b10b593-0648-4ae6-825f-704cce381080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
