{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de207c6",
   "metadata": {},
   "source": [
    "# Rethinking Graph Neural Networks for Anomaly Detection\n",
    "* 图神经网络\n",
    "* 小波变换\n",
    "* 异常检测\n",
    "* https://github.com/squareRoot3/Rethinking-Anomaly-Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8fa06",
   "metadata": {},
   "source": [
    "## 改进思路\n",
    "### 1. 编码度信息到attribute of node\n",
    "### 2. 将图节点的度编码为one hot 编码，通过注意力网络或者图卷积神经网络进行训练，并与原模型的output进行拼合\n",
    "###  <font color= 'Red'> 3. 用haar，Mexican-Hat 和Meyer小波结果进行比较</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638e8ab3",
   "metadata": {},
   "source": [
    "## 贝塔函数 B(α,β) 的另一种常见形式:   B(α,β)= Γ(α)Γ(β)/Γ(α+β)\n",
    "## Γ(x)=(x−1)!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef142eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# 其中包括激活函数, 损失函数, 池化函数 ,通过 F.xxx() 的形式，可以方便地调用 torch.nn.functional 模块中的各种函数\n",
    "import numpy\n",
    "import argparse\n",
    "import time\n",
    "from dataset_process.dataset import Dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, roc_auc_score, precision_score, confusion_matrix\n",
    "from model.BWGNN import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed55746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, args):\n",
    "    features = g.ndata['feature']\n",
    "    labels = g.ndata['label']\n",
    "    index = list(range(len(labels)))\n",
    "    if dataset_name == 'amazon':\n",
    "        index = list(range(3305, len(labels)))\n",
    "\n",
    "    idx_train, idx_rest, y_train, y_rest = train_test_split(index, labels[index], stratify=labels[index],\n",
    "                                                            train_size=args.train_ratio,\n",
    "                                                            random_state=2, shuffle=True)\n",
    "    idx_valid, idx_test, y_valid, y_test = train_test_split(idx_rest, y_rest, stratify=y_rest,\n",
    "                                                            test_size=0.67,\n",
    "                                                            random_state=2, shuffle=True)\n",
    "    train_mask = torch.zeros([len(labels)]).bool()\n",
    "    val_mask = torch.zeros([len(labels)]).bool()\n",
    "    test_mask = torch.zeros([len(labels)]).bool()\n",
    "\n",
    "    train_mask[idx_train] = 1\n",
    "    val_mask[idx_valid] = 1\n",
    "    test_mask[idx_test] = 1\n",
    "    print('train/dev/test samples: ', train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_f1, final_tf1, final_trec, final_tpre, final_tmf1, final_tauc = 0., 0., 0., 0., 0., 0.\n",
    "\n",
    "    weight = (1-labels[train_mask]).sum().item() / labels[train_mask].sum().item()\n",
    "    print('cross entropy weight: ', weight)\n",
    "    time_start = time.time()\n",
    "    for e in range(args.epoch):\n",
    "        # 训练\n",
    "        model.train()\n",
    "        # 调用模型中的forward函数\n",
    "        logits = model(features)\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask], weight=torch.tensor([1., weight]))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #验证\n",
    "        model.eval()\n",
    "        probs = logits.softmax(1)\n",
    "        f1, thres = get_best_f1(labels[val_mask], probs[val_mask])\n",
    "        preds = numpy.zeros_like(labels)\n",
    "        preds[probs[:, 1] > thres] = 1\n",
    "        trec = recall_score(labels[test_mask], preds[test_mask])\n",
    "        tpre = precision_score(labels[test_mask], preds[test_mask])\n",
    "        tmf1 = f1_score(labels[test_mask], preds[test_mask], average='macro')\n",
    "        tauc = roc_auc_score(labels[test_mask], probs[test_mask][:, 1].detach().numpy())\n",
    "\n",
    "        if best_f1 < f1:\n",
    "            best_f1 = f1\n",
    "            final_trec = trec\n",
    "            final_tpre = tpre\n",
    "            final_tmf1 = tmf1\n",
    "            final_tauc = tauc\n",
    "        print('Epoch {}, loss: {:.4f}, val mf1: {:.4f}, (best {:.4f})'.format(e, loss, f1, best_f1))\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('time cost: ', time_end - time_start, 's')\n",
    "    print('Test: REC {:.2f} PRE {:.2f} MF1 {:.2f} AUC {:.2f}'.format(final_trec*100,\n",
    "                                                                     final_tpre*100, final_tmf1*100, final_tauc*100))\n",
    "    return final_tmf1, final_tauc\n",
    "\n",
    "\n",
    "# threshold adjusting for best macro f1\n",
    "def get_best_f1(labels, probs):\n",
    "    best_f1, best_thre = 0, 0\n",
    "    for thres in np.linspace(0.05, 0.95, 19):\n",
    "        #构建一个与labels同维度的数组,并初始化所有变量为零\n",
    "        preds = np.zeros_like(labels)\n",
    "        preds[probs[:,1] > thres] = 1\n",
    "        #average='binary'：计算二分类问题中的 F1 分数（默认值）。\n",
    "        #average='micro'：对所有类别的真实和预测样本进行汇总，然后计算 F1 分数。\n",
    "        #average='macro'：计算每个类别的 F1 分数，然后取平均值。\n",
    "        #average=None：返回每个类别的 F1 分数。\n",
    "        # F1_score 详细原理间“备份”\n",
    "        mf1 = f1_score(labels, preds, average='macro')\n",
    "        if mf1 > best_f1:\n",
    "            best_f1 = mf1\n",
    "            best_thre = thres\n",
    "    return best_f1, best_thre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89edfe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='yelp', train_ratio=0.4, hid_dim=64, order=2, homo=0, epoch=100, run=1)\n",
      "Done loading data from cached files.\n",
      "Graph(num_nodes={'review': 45954},\n",
      "      num_edges={('review', 'net_rsr', 'review'): 6805486, ('review', 'net_rtr', 'review'): 1147232, ('review', 'net_rur', 'review'): 98630},\n",
      "      metagraph=[('review', 'review', 'net_rsr'), ('review', 'review', 'net_rtr'), ('review', 'review', 'net_rur')])\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='BWGNN')\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"yelp\",\n",
    "                        help=\"Dataset for this model (yelp/amazon/tfinance/tsocial)\")\n",
    "parser.add_argument(\"--train_ratio\", type=float, default=0.40, help=\"Training ratio\")\n",
    "parser.add_argument(\"--hid_dim\", type=int, default=64, help=\"Hidden layer dimension\")\n",
    "# \"Order C in Beta Wavelet\"  P + q = C ：order.  Beta 分布的概率密度函数中的两个重要参数\n",
    "parser.add_argument(\"--order\", type=int, default=2, help=\"Order C in Beta Wavelet\")\n",
    "parser.add_argument(\"--homo\", type=int, default=0, help=\"1 for BWGNN(Homo) and 0 for BWGNN(Hetero)\")\n",
    "parser.add_argument(\"--epoch\", type=int, default=100, help=\"The max number of epochs\")\n",
    "parser.add_argument(\"--run\", type=int, default=1, help=\"Running times\")\n",
    "\n",
    "\n",
    "args = parser.parse_args(args = [])\n",
    "print(args)\n",
    "dataset_name = args.dataset\n",
    "homo = args.homo\n",
    "order = args.order\n",
    "h_feats = args.hid_dim\n",
    "graph = Dataset(dataset_name, homo).graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c46ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################test############################\n",
    "print(graph.num_nodes())   # 获取图的节点数 ：11944\n",
    "#print(graph.ndata)\n",
    "# graph.ndata['feature'] 11944*25\n",
    "print(graph.ndata['label'].shape)\n",
    "print(graph.num_edges())  #获取图的边的数目：9569592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb2d6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 192])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([2, 64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([2])\n",
      "train/dev/test samples:  18381 9099 18474\n",
      "cross entropy weight:  5.8816922500935975\n",
      "Epoch 0, loss: 0.6908, val mf1: 0.4614, (best 0.4614)\n",
      "Epoch 1, loss: 0.6598, val mf1: 0.5556, (best 0.5556)\n",
      "Epoch 2, loss: 1.0537, val mf1: 0.6001, (best 0.6001)\n",
      "Epoch 3, loss: 0.6582, val mf1: 0.5433, (best 0.6001)\n",
      "Epoch 4, loss: 0.6311, val mf1: 0.5625, (best 0.6001)\n",
      "Epoch 5, loss: 0.6100, val mf1: 0.5713, (best 0.6001)\n",
      "Epoch 6, loss: 0.6014, val mf1: 0.6405, (best 0.6405)\n",
      "Epoch 7, loss: 0.6013, val mf1: 0.6506, (best 0.6506)\n",
      "Epoch 8, loss: 0.5937, val mf1: 0.6533, (best 0.6533)\n",
      "Epoch 9, loss: 0.5859, val mf1: 0.6585, (best 0.6585)\n",
      "Epoch 10, loss: 0.5804, val mf1: 0.6612, (best 0.6612)\n",
      "Epoch 11, loss: 0.5660, val mf1: 0.6666, (best 0.6666)\n",
      "Epoch 12, loss: 0.5540, val mf1: 0.6765, (best 0.6765)\n",
      "Epoch 13, loss: 0.5630, val mf1: 0.6837, (best 0.6837)\n",
      "Epoch 14, loss: 0.5650, val mf1: 0.6829, (best 0.6837)\n",
      "Epoch 15, loss: 0.5223, val mf1: 0.6868, (best 0.6868)\n",
      "Epoch 16, loss: 0.5323, val mf1: 0.6940, (best 0.6940)\n",
      "Epoch 17, loss: 0.5201, val mf1: 0.6931, (best 0.6940)\n",
      "Epoch 18, loss: 0.5020, val mf1: 0.6948, (best 0.6948)\n",
      "Epoch 19, loss: 0.5002, val mf1: 0.6911, (best 0.6948)\n",
      "Epoch 20, loss: 0.5005, val mf1: 0.6916, (best 0.6948)\n",
      "Epoch 21, loss: 0.4960, val mf1: 0.6991, (best 0.6991)\n",
      "Epoch 22, loss: 0.4891, val mf1: 0.7023, (best 0.7023)\n",
      "Epoch 23, loss: 0.4875, val mf1: 0.7049, (best 0.7049)\n",
      "Epoch 24, loss: 0.4855, val mf1: 0.7032, (best 0.7049)\n",
      "Epoch 25, loss: 0.4789, val mf1: 0.7064, (best 0.7064)\n",
      "Epoch 26, loss: 0.4755, val mf1: 0.7076, (best 0.7076)\n",
      "Epoch 27, loss: 0.4707, val mf1: 0.7074, (best 0.7076)\n",
      "Epoch 28, loss: 0.4675, val mf1: 0.7117, (best 0.7117)\n",
      "Epoch 29, loss: 0.4631, val mf1: 0.7115, (best 0.7117)\n",
      "Epoch 30, loss: 0.4595, val mf1: 0.7166, (best 0.7166)\n",
      "Epoch 31, loss: 0.4541, val mf1: 0.7224, (best 0.7224)\n",
      "Epoch 32, loss: 0.4528, val mf1: 0.7158, (best 0.7224)\n",
      "Epoch 33, loss: 0.4500, val mf1: 0.7270, (best 0.7270)\n",
      "Epoch 34, loss: 0.4494, val mf1: 0.7163, (best 0.7270)\n",
      "Epoch 35, loss: 0.4510, val mf1: 0.7292, (best 0.7292)\n",
      "Epoch 36, loss: 0.4538, val mf1: 0.7147, (best 0.7292)\n",
      "Epoch 37, loss: 0.4331, val mf1: 0.7337, (best 0.7337)\n",
      "Epoch 38, loss: 0.4347, val mf1: 0.7305, (best 0.7337)\n",
      "Epoch 39, loss: 0.4379, val mf1: 0.7274, (best 0.7337)\n",
      "Epoch 40, loss: 0.4219, val mf1: 0.7357, (best 0.7357)\n",
      "Epoch 41, loss: 0.4317, val mf1: 0.7389, (best 0.7389)\n",
      "Epoch 42, loss: 0.4197, val mf1: 0.7373, (best 0.7389)\n",
      "Epoch 43, loss: 0.4227, val mf1: 0.7333, (best 0.7389)\n",
      "Epoch 44, loss: 0.4141, val mf1: 0.7437, (best 0.7437)\n",
      "Epoch 45, loss: 0.4132, val mf1: 0.7440, (best 0.7440)\n",
      "Epoch 46, loss: 0.4070, val mf1: 0.7425, (best 0.7440)\n",
      "Epoch 47, loss: 0.4058, val mf1: 0.7457, (best 0.7457)\n",
      "Epoch 48, loss: 0.4015, val mf1: 0.7508, (best 0.7508)\n",
      "Epoch 49, loss: 0.4004, val mf1: 0.7526, (best 0.7526)\n",
      "Epoch 50, loss: 0.4021, val mf1: 0.7438, (best 0.7526)\n",
      "Epoch 51, loss: 0.4043, val mf1: 0.7481, (best 0.7526)\n",
      "Epoch 52, loss: 0.3984, val mf1: 0.7519, (best 0.7526)\n",
      "Epoch 53, loss: 0.3867, val mf1: 0.7593, (best 0.7593)\n",
      "Epoch 54, loss: 0.3818, val mf1: 0.7583, (best 0.7593)\n",
      "Epoch 55, loss: 0.3853, val mf1: 0.7528, (best 0.7593)\n",
      "Epoch 56, loss: 0.3818, val mf1: 0.7587, (best 0.7593)\n",
      "Epoch 57, loss: 0.3724, val mf1: 0.7616, (best 0.7616)\n",
      "Epoch 58, loss: 0.3690, val mf1: 0.7590, (best 0.7616)\n",
      "Epoch 59, loss: 0.3726, val mf1: 0.7663, (best 0.7663)\n",
      "Epoch 60, loss: 0.3748, val mf1: 0.7660, (best 0.7663)\n",
      "Epoch 61, loss: 0.3878, val mf1: 0.7619, (best 0.7663)\n",
      "Epoch 62, loss: 0.3781, val mf1: 0.7662, (best 0.7663)\n",
      "Epoch 63, loss: 0.3626, val mf1: 0.7625, (best 0.7663)\n",
      "Epoch 64, loss: 0.3568, val mf1: 0.7641, (best 0.7663)\n",
      "Epoch 65, loss: 0.3609, val mf1: 0.7671, (best 0.7671)\n",
      "Epoch 66, loss: 0.3537, val mf1: 0.7694, (best 0.7694)\n",
      "Epoch 67, loss: 0.3495, val mf1: 0.7707, (best 0.7707)\n",
      "Epoch 68, loss: 0.3511, val mf1: 0.7727, (best 0.7727)\n",
      "Epoch 69, loss: 0.3418, val mf1: 0.7762, (best 0.7762)\n",
      "Epoch 70, loss: 0.3421, val mf1: 0.7728, (best 0.7762)\n",
      "Epoch 71, loss: 0.3400, val mf1: 0.7745, (best 0.7762)\n",
      "Epoch 72, loss: 0.3354, val mf1: 0.7774, (best 0.7774)\n",
      "Epoch 73, loss: 0.3327, val mf1: 0.7758, (best 0.7774)\n",
      "Epoch 74, loss: 0.3297, val mf1: 0.7778, (best 0.7778)\n",
      "Epoch 75, loss: 0.3268, val mf1: 0.7771, (best 0.7778)\n",
      "Epoch 76, loss: 0.3253, val mf1: 0.7778, (best 0.7778)\n",
      "Epoch 77, loss: 0.3239, val mf1: 0.7774, (best 0.7778)\n",
      "Epoch 78, loss: 0.3251, val mf1: 0.7780, (best 0.7780)\n",
      "Epoch 79, loss: 0.3377, val mf1: 0.7748, (best 0.7780)\n",
      "Epoch 80, loss: 0.3518, val mf1: 0.7770, (best 0.7780)\n",
      "Epoch 81, loss: 0.3959, val mf1: 0.7680, (best 0.7780)\n",
      "Epoch 82, loss: 0.3200, val mf1: 0.7728, (best 0.7780)\n",
      "Epoch 83, loss: 0.3698, val mf1: 0.7682, (best 0.7780)\n",
      "Epoch 84, loss: 0.3257, val mf1: 0.7694, (best 0.7780)\n",
      "Epoch 85, loss: 0.3497, val mf1: 0.7620, (best 0.7780)\n",
      "Epoch 86, loss: 0.3355, val mf1: 0.7609, (best 0.7780)\n",
      "Epoch 87, loss: 0.3229, val mf1: 0.7736, (best 0.7780)\n",
      "Epoch 88, loss: 0.3355, val mf1: 0.7699, (best 0.7780)\n",
      "Epoch 89, loss: 0.3174, val mf1: 0.7777, (best 0.7780)\n",
      "Epoch 90, loss: 0.3223, val mf1: 0.7656, (best 0.7780)\n",
      "Epoch 91, loss: 0.3171, val mf1: 0.7710, (best 0.7780)\n",
      "Epoch 92, loss: 0.3078, val mf1: 0.7778, (best 0.7780)\n",
      "Epoch 93, loss: 0.3137, val mf1: 0.7752, (best 0.7780)\n",
      "Epoch 94, loss: 0.3034, val mf1: 0.7807, (best 0.7807)\n",
      "Epoch 95, loss: 0.3043, val mf1: 0.7749, (best 0.7807)\n",
      "Epoch 96, loss: 0.2967, val mf1: 0.7778, (best 0.7807)\n",
      "Epoch 97, loss: 0.2964, val mf1: 0.7792, (best 0.7807)\n",
      "Epoch 98, loss: 0.2900, val mf1: 0.7796, (best 0.7807)\n",
      "Epoch 99, loss: 0.2879, val mf1: 0.7805, (best 0.7807)\n",
      "time cost:  637.9715707302094 s\n",
      "Test: REC 61.55 PRE 61.34 MF1 77.44 AUC 90.42\n",
      "MF1-mean: 77.44, MF1-std: 0.00, AUC-mean: 90.42, AUC-std: 0.00\n"
     ]
    }
   ],
   "source": [
    "in_feats = graph.ndata['feature'].shape[1]\n",
    "num_classes = 2\n",
    "\n",
    "if args.run == 0:\n",
    "    if homo:\n",
    "        print(\"hello\")\n",
    "        model = BWGNN(in_feats, h_feats, num_classes, graph, d=order)\n",
    "    else:\n",
    "        model = BWGNN_Hetero(in_feats, h_feats, num_classes, graph, d=order)\n",
    "        train(model, graph, args)\n",
    "\n",
    "else:\n",
    "    final_mf1s, final_aucs = [], []\n",
    "    for tt in range(args.run):\n",
    "        if homo:\n",
    "            #in_feats 特征点维度；h_feats：隐层维度；num_classes：节点分类数（nomal，anomaly）\n",
    "            model = BWGNN(in_feats, h_feats, num_classes, graph, d=order)\n",
    "        else:\n",
    "            model = BWGNN_Hetero(in_feats, h_feats, num_classes, graph, d=order)\n",
    "        mf1, auc = train(model, graph, args)\n",
    "        final_mf1s.append(mf1)\n",
    "        final_aucs.append(auc)\n",
    "    final_mf1s = np.array(final_mf1s)\n",
    "    final_aucs = np.array(final_aucs)\n",
    "    # np.std :计算全局标准差\n",
    "    print('MF1-mean: {:.2f}, MF1-std: {:.2f}, AUC-mean: {:.2f}, AUC-std: {:.2f}'.format(100 * np.mean(final_mf1s),\n",
    "                                                                                            100 * np.std(final_mf1s),\n",
    "                                                               100 * np.mean(final_aucs), 100 * np.std(final_aucs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d4df4",
   "metadata": {},
   "source": [
    "## Dataset： \"yelp\" homo\n",
    "#### train/dev/test samples:  18381 9099 18474\n",
    "#### cross entropy weight:  5.8816922500935975\n",
    "#### D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
    "####   _warn_prf(average, modifier, msg_start, len(result))\n",
    "#### Epoch 0, loss: 0.6964, val mf1: 0.4608, (best 0.4608)\n",
    "#### Epoch 1, loss: 0.7586, val mf1: 0.5158, (best 0.5158)\n",
    "#### Epoch 2, loss: 0.6847, val mf1: 0.5712, (best 0.5712)\n",
    "#### Epoch 3, loss: 0.6877, val mf1: 0.5891, (best 0.5891)\n",
    "#### Epoch 4, loss: 0.6785, val mf1: 0.5564, (best 0.5891)\n",
    "#### Epoch 5, loss: 0.6656, val mf1: 0.5962, (best 0.5962)\n",
    "#### Epoch 6, loss: 0.6491, val mf1: 0.6016, (best 0.6016)\n",
    "#### Epoch 7, loss: 0.6300, val mf1: 0.6065, (best 0.6065)\n",
    "#### Epoch 8, loss: 0.6113, val mf1: 0.6150, (best 0.6150)\n",
    "#### Epoch 9, loss: 0.6058, val mf1: 0.6238, (best 0.6238)\n",
    "#### Epoch 10, loss: 0.6043, val mf1: 0.6254, (best 0.6254)\n",
    "#### Epoch 11, loss: 0.6010, val mf1: 0.6292, (best 0.6292)\n",
    "#### Epoch 12, loss: 0.5953, val mf1: 0.6289, (best 0.6292)\n",
    "#### Epoch 13, loss: 0.5920, val mf1: 0.6310, (best 0.6310)\n",
    "#### Epoch 14, loss: 0.5909, val mf1: 0.6350, (best 0.6350)\n",
    "#### Epoch 15, loss: 0.5883, val mf1: 0.6357, (best 0.6357)\n",
    "#### Epoch 16, loss: 0.5870, val mf1: 0.6354, (best 0.6357)\n",
    "#### Epoch 17, loss: 0.5847, val mf1: 0.6386, (best 0.6386)\n",
    "#### Epoch 18, loss: 0.5845, val mf1: 0.6422, (best 0.6422)\n",
    "#### Epoch 19, loss: 0.5839, val mf1: 0.6412, (best 0.6422)\n",
    "#### Epoch 20, loss: 0.5823, val mf1: 0.6434, (best 0.6434)\n",
    "#### Epoch 21, loss: 0.5807, val mf1: 0.6454, (best 0.6454)\n",
    "#### Epoch 22, loss: 0.5789, val mf1: 0.6471, (best 0.6471)\n",
    "#### Epoch 23, loss: 0.5780, val mf1: 0.6493, (best 0.6493)\n",
    "#### Epoch 24, loss: 0.5779, val mf1: 0.6469, (best 0.6493)\n",
    "#### Epoch 25, loss: 0.5770, val mf1: 0.6470, (best 0.6493)\n",
    "#### Epoch 26, loss: 0.5753, val mf1: 0.6482, (best 0.6493)\n",
    "#### Epoch 27, loss: 0.5737, val mf1: 0.6482, (best 0.6493)\n",
    "#### Epoch 28, loss: 0.5722, val mf1: 0.6479, (best 0.6493)\n",
    "#### Epoch 29, loss: 0.5713, val mf1: 0.6516, (best 0.6516)\n",
    "#### Epoch 30, loss: 0.5735, val mf1: 0.6504, (best 0.6516)\n",
    "#### Epoch 31, loss: 0.5880, val mf1: 0.6514, (best 0.6516)\n",
    "#### Epoch 32, loss: 0.5829, val mf1: 0.6508, (best 0.6516)\n",
    "#### Epoch 33, loss: 0.5715, val mf1: 0.6505, (best 0.6516)\n",
    "#### Epoch 34, loss: 0.5782, val mf1: 0.6526, (best 0.6526)\n",
    "#### Epoch 35, loss: 0.5682, val mf1: 0.6541, (best 0.6541)\n",
    "#### Epoch 36, loss: 0.5706, val mf1: 0.6514, (best 0.6541)\n",
    "#### Epoch 37, loss: 0.5692, val mf1: 0.6535, (best 0.6541)\n",
    "#### Epoch 38, loss: 0.5644, val mf1: 0.6538, (best 0.6541)\n",
    "#### Epoch 39, loss: 0.5691, val mf1: 0.6569, (best 0.6569)\n",
    "#### Epoch 40, loss: 0.5611, val mf1: 0.6539, (best 0.6569)\n",
    "#### Epoch 41, loss: 0.5647, val mf1: 0.6513, (best 0.6569)\n",
    "#### Epoch 42, loss: 0.5618, val mf1: 0.6558, (best 0.6569)\n",
    "#### Epoch 43, loss: 0.5595, val mf1: 0.6557, (best 0.6569)\n",
    "#### Epoch 44, loss: 0.5612, val mf1: 0.6559, (best 0.6569)\n",
    "#### Epoch 45, loss: 0.5562, val mf1: 0.6562, (best 0.6569)\n",
    "#### Epoch 46, loss: 0.5576, val mf1: 0.6573, (best 0.6573)\n",
    "#### Epoch 47, loss: 0.5553, val mf1: 0.6586, (best 0.6586)\n",
    "#### Epoch 48, loss: 0.5538, val mf1: 0.6591, (best 0.6591)\n",
    "#### Epoch 49, loss: 0.5528, val mf1: 0.6587, (best 0.6591)\n",
    "#### Epoch 50, loss: 0.5502, val mf1: 0.6581, (best 0.6591)\n",
    "#### Epoch 51, loss: 0.5508, val mf1: 0.6555, (best 0.6591)\n",
    "#### Epoch 52, loss: 0.5474, val mf1: 0.6584, (best 0.6591)\n",
    "#### Epoch 53, loss: 0.5474, val mf1: 0.6588, (best 0.6591)\n",
    "#### Epoch 54, loss: 0.5442, val mf1: 0.6618, (best 0.6618)\n",
    "#### Epoch 55, loss: 0.5445, val mf1: 0.6629, (best 0.6629)\n",
    "#### Epoch 56, loss: 0.5413, val mf1: 0.6632, (best 0.6632)\n",
    "#### Epoch 57, loss: 0.5409, val mf1: 0.6627, (best 0.6632)\n",
    "#### Epoch 58, loss: 0.5398, val mf1: 0.6633, (best 0.6633)\n",
    "#### Epoch 59, loss: 0.5367, val mf1: 0.6619, (best 0.6633)\n",
    "#### Epoch 60, loss: 0.5367, val mf1: 0.6634, (best 0.6634)\n",
    "#### Epoch 61, loss: 0.5374, val mf1: 0.6655, (best 0.6655)\n",
    "#### Epoch 62, loss: 0.5367, val mf1: 0.6658, (best 0.6658)\n",
    "#### Epoch 63, loss: 0.5357, val mf1: 0.6669, (best 0.6669)\n",
    "#### Epoch 64, loss: 0.5351, val mf1: 0.6652, (best 0.6669)\n",
    "#### Epoch 65, loss: 0.5352, val mf1: 0.6694, (best 0.6694)\n",
    "#### Epoch 66, loss: 0.5372, val mf1: 0.6654, (best 0.6694)\n",
    "#### Epoch 67, loss: 0.5409, val mf1: 0.6665, (best 0.6694)\n",
    "#### Epoch 68, loss: 0.5367, val mf1: 0.6682, (best 0.6694)\n",
    "#### Epoch 69, loss: 0.5281, val mf1: 0.6656, (best 0.6694)\n",
    "#### Epoch 70, loss: 0.5243, val mf1: 0.6680, (best 0.6694)\n",
    "#### Epoch 71, loss: 0.5286, val mf1: 0.6680, (best 0.6694)\n",
    "#### Epoch 72, loss: 0.5301, val mf1: 0.6689, (best 0.6694)\n",
    "#### Epoch 73, loss: 0.5233, val mf1: 0.6696, (best 0.6696)\n",
    "#### Epoch 74, loss: 0.5209, val mf1: 0.6688, (best 0.6696)\n",
    "#### Epoch 75, loss: 0.5244, val mf1: 0.6690, (best 0.6696)\n",
    "#### Epoch 76, loss: 0.5240, val mf1: 0.6724, (best 0.6724)\n",
    "#### Epoch 77, loss: 0.5193, val mf1: 0.6694, (best 0.6724)\n",
    "#### Epoch 78, loss: 0.5166, val mf1: 0.6710, (best 0.6724)\n",
    "#### Epoch 79, loss: 0.5186, val mf1: 0.6713, (best 0.6724)\n",
    "#### Epoch 80, loss: 0.5221, val mf1: 0.6702, (best 0.6724)\n",
    "#### Epoch 81, loss: 0.5229, val mf1: 0.6711, (best 0.6724)\n",
    "#### Epoch 82, loss: 0.5208, val mf1: 0.6715, (best 0.6724)\n",
    "#### Epoch 83, loss: 0.5148, val mf1: 0.6750, (best 0.6750)\n",
    "#### Epoch 84, loss: 0.5107, val mf1: 0.6726, (best 0.6750)\n",
    "#### Epoch 85, loss: 0.5105, val mf1: 0.6741, (best 0.6750)\n",
    "#### Epoch 86, loss: 0.5130, val mf1: 0.6759, (best 0.6759)\n",
    "#### Epoch 87, loss: 0.5181, val mf1: 0.6736, (best 0.6759)\n",
    "#### Epoch 88, loss: 0.5220, val mf1: 0.6812, (best 0.6812)\n",
    "#### Epoch 89, loss: 0.5244, val mf1: 0.6737, (best 0.6812)\n",
    "#### Epoch 90, loss: 0.5117, val mf1: 0.6765, (best 0.6812)\n",
    "#### Epoch 91, loss: 0.5049, val mf1: 0.6784, (best 0.6812)\n",
    "#### Epoch 92, loss: 0.5103, val mf1: 0.6773, (best 0.6812)\n",
    "#### Epoch 93, loss: 0.5106, val mf1: 0.6796, (best 0.6812)\n",
    "#### Epoch 94, loss: 0.5041, val mf1: 0.6793, (best 0.6812)\n",
    "#### Epoch 95, loss: 0.5033, val mf1: 0.6779, (best 0.6812)\n",
    "#### Epoch 96, loss: 0.5065, val mf1: 0.6792, (best 0.6812)\n",
    "#### Epoch 97, loss: 0.5041, val mf1: 0.6793, (best 0.6812)\n",
    "#### Epoch 98, loss: 0.4992, val mf1: 0.6797, (best 0.6812)\n",
    "#### Epoch 99, loss: 0.5007, val mf1: 0.6798, (best 0.6812)\n",
    "#### time cost:  1497.5507764816284 s\n",
    "#### Test: REC 52.68 PRE 42.58 MF1 68.41 AUC 81.70\n",
    "#### MF1-mean: 68.41, MF1-std: 0.00, AUC-mean: 81.70, AUC-std: 0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b5d350",
   "metadata": {},
   "source": [
    "## Dataset： \"yelp\" hetor\n",
    "#### <class 'torch.nn.parameter.Parameter'> torch.Size([64, 32])\n",
    "#### class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
    "#### <class 'torch.nn.parameter.Parameter'> torch.Size([64, 64])\n",
    "#### <class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
    "#### <class 'torch.nn.parameter.Parameter'> torch.Size([64, 192])\n",
    "#### <class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
    "#### <class 'torch.nn.parameter.Parameter'> torch.Size([2, 64])\n",
    "#### <class 'torch.nn.parameter.Parameter'> torch.Size([2])\n",
    "#### train/dev/test samples:  18381 9099 18474\n",
    "#### cross entropy weight:  5.8816922500935975\n",
    "#### Epoch 0, loss: 0.6865, val mf1: 0.4912, (best 0.4912)\n",
    "#### Epoch 1, loss: 0.6184, val mf1: 0.6001, (best 0.6001)\n",
    "#### Epoch 2, loss: 1.2713, val mf1: 0.5448, (best 0.6001)\n",
    "#### Epoch 3, loss: 0.6101, val mf1: 0.5555, (best 0.6001)\n",
    "#### Epoch 4, loss: 0.6484, val mf1: 0.5432, (best 0.6001)\n",
    "#### Epoch 5, loss: 0.6374, val mf1: 0.5536, (best 0.6001)\n",
    "#### Epoch 6, loss: 0.6297, val mf1: 0.5434, (best 0.6001)\n",
    "#### Epoch 7, loss: 0.6249, val mf1: 0.5534, (best 0.6001)\n",
    "#### Epoch 8, loss: 0.6102, val mf1: 0.5785, (best 0.6001)\n",
    "#### Epoch 9, loss: 0.6031, val mf1: 0.5487, (best 0.6001)\n",
    "#### Epoch 10, loss: 0.6016, val mf1: 0.5666, (best 0.6001)\n",
    "#### Epoch 11, loss: 0.5855, val mf1: 0.6447, (best 0.6447)\n",
    "#### Epoch 12, loss: 0.5717, val mf1: 0.6511, (best 0.6511)\n",
    "#### Epoch 13, loss: 0.5531, val mf1: 0.6618, (best 0.6618)\n",
    "#### Epoch 14, loss: 0.5444, val mf1: 0.6680, (best 0.6680)\n",
    "#### Epoch 15, loss: 0.5255, val mf1: 0.6785, (best 0.6785)\n",
    "#### Epoch 16, loss: 0.5264, val mf1: 0.6833, (best 0.6833)\n",
    "#### Epoch 17, loss: 0.5123, val mf1: 0.6926, (best 0.6926)\n",
    "#### Epoch 18, loss: 0.5066, val mf1: 0.6950, (best 0.6950)\n",
    "#### Epoch 19, loss: 0.5007, val mf1: 0.6929, (best 0.6950)\n",
    "#### Epoch 20, loss: 0.4962, val mf1: 0.6954, (best 0.6954)\n",
    "#### Epoch 21, loss: 0.4900, val mf1: 0.6973, (best 0.6973)\n",
    "#### Epoch 22, loss: 0.4876, val mf1: 0.7012, (best 0.7012)\n",
    "#### Epoch 23, loss: 0.4814, val mf1: 0.7059, (best 0.7059)\n",
    "#### Epoch 24, loss: 0.4779, val mf1: 0.7062, (best 0.7062)\n",
    "#### Epoch 25, loss: 0.4720, val mf1: 0.7115, (best 0.7115)\n",
    "#### Epoch 26, loss: 0.4677, val mf1: 0.7169, (best 0.7169)\n",
    "#### Epoch 27, loss: 0.4675, val mf1: 0.7201, (best 0.7201)\n",
    "#### Epoch 28, loss: 0.4993, val mf1: 0.7236, (best 0.7236)\n",
    "#### Epoch 29, loss: 0.5034, val mf1: 0.7242, (best 0.7242)\n",
    "#### Epoch 30, loss: 0.4594, val mf1: 0.7210, (best 0.7242)\n",
    "#### Epoch 31, loss: 0.4659, val mf1: 0.7209, (best 0.7242)\n",
    "#### Epoch 32, loss: 0.4654, val mf1: 0.7198, (best 0.7242)\n",
    "#### Epoch 33, loss: 0.4548, val mf1: 0.7199, (best 0.7242)\n",
    "#### Epoch 34, loss: 0.4529, val mf1: 0.7154, (best 0.7242)\n",
    "#### Epoch 35, loss: 0.4589, val mf1: 0.7106, (best 0.7242)\n",
    "#### Epoch 36, loss: 0.4514, val mf1: 0.7183, (best 0.7242)\n",
    "#### Epoch 37, loss: 0.4471, val mf1: 0.7248, (best 0.7248)\n",
    "#### Epoch 38, loss: 0.4435, val mf1: 0.7262, (best 0.7262)\n",
    "#### Epoch 39, loss: 0.4446, val mf1: 0.7285, (best 0.7285)\n",
    "#### Epoch 40, loss: 0.4394, val mf1: 0.7300, (best 0.7300)\n",
    "#### Epoch 41, loss: 0.4349, val mf1: 0.7314, (best 0.7314)\n",
    "#### Epoch 42, loss: 0.4320, val mf1: 0.7301, (best 0.7314)\n",
    "#### Epoch 43, loss: 0.4297, val mf1: 0.7317, (best 0.7317)\n",
    "#### Epoch 44, loss: 0.4255, val mf1: 0.7351, (best 0.7351)\n",
    "#### Epoch 45, loss: 0.4207, val mf1: 0.7375, (best 0.7375)\n",
    "#### Epoch 46, loss: 0.4175, val mf1: 0.7383, (best 0.7383)\n",
    "#### Epoch 47, loss: 0.4131, val mf1: 0.7429, (best 0.7429)\n",
    "#### Epoch 48, loss: 0.4100, val mf1: 0.7428, (best 0.7429)\n",
    "#### Epoch 49, loss: 0.4057, val mf1: 0.7461, (best 0.7461)\n",
    "#### Epoch 50, loss: 0.4018, val mf1: 0.7476, (best 0.7476)\n",
    "#### Epoch 51, loss: 0.3984, val mf1: 0.7494, (best 0.7494)\n",
    "#### Epoch 52, loss: 0.3944, val mf1: 0.7520, (best 0.7520)\n",
    "#### Epoch 53, loss: 0.3908, val mf1: 0.7521, (best 0.7521)\n",
    "#### Epoch 54, loss: 0.3888, val mf1: 0.7554, (best 0.7554)\n",
    "#### Epoch 55, loss: 0.3929, val mf1: 0.7566, (best 0.7566)\n",
    "#### Epoch 56, loss: 0.4520, val mf1: 0.7535, (best 0.7566)\n",
    "#### Epoch 57, loss: 0.5836, val mf1: 0.7376, (best 0.7566)\n",
    "#### Epoch 58, loss: 0.3963, val mf1: 0.7462, (best 0.7566)\n",
    "#### Epoch 59, loss: 0.4298, val mf1: 0.7526, (best 0.7566)\n",
    "#### Epoch 60, loss: 0.4261, val mf1: 0.7526, (best 0.7566)\n",
    "#### Epoch 61, loss: 0.4222, val mf1: 0.7503, (best 0.7566)\n",
    "#### Epoch 62, loss: 0.4239, val mf1: 0.7454, (best 0.7566)\n",
    "#### Epoch 63, loss: 0.4246, val mf1: 0.7379, (best 0.7566)\n",
    "#### Epoch 64, loss: 0.4160, val mf1: 0.7435, (best 0.7566)\n",
    "#### Epoch 65, loss: 0.4203, val mf1: 0.7421, (best 0.7566)\n",
    "#### Epoch 66, loss: 0.4101, val mf1: 0.7408, (best 0.7566)\n",
    "#### Epoch 67, loss: 0.4075, val mf1: 0.7413, (best 0.7566)\n",
    "#### Epoch 68, loss: 0.4065, val mf1: 0.7464, (best 0.7566)\n",
    "#### Epoch 69, loss: 0.3996, val mf1: 0.7524, (best 0.7566)\n",
    "#### Epoch 70, loss: 0.3999, val mf1: 0.7492, (best 0.7566)\n",
    "#### Epoch 71, loss: 0.3955, val mf1: 0.7512, (best 0.7566)\n",
    "#### Epoch 72, loss: 0.3921, val mf1: 0.7535, (best 0.7566)\n",
    "#### Epoch 73, loss: 0.3865, val mf1: 0.7546, (best 0.7566)\n",
    "#### Epoch 74, loss: 0.3858, val mf1: 0.7564, (best 0.7566)\n",
    "#### Epoch 75, loss: 0.3825, val mf1: 0.7577, (best 0.7577)\n",
    "#### Epoch 76, loss: 0.3787, val mf1: 0.7596, (best 0.7596)\n",
    "#### Epoch 77, loss: 0.3739, val mf1: 0.7611, (best 0.7611)\n",
    "#### Epoch 78, loss: 0.3725, val mf1: 0.7628, (best 0.7628)\n",
    "#### Epoch 79, loss: 0.3698, val mf1: 0.7660, (best 0.7660)\n",
    "#### Epoch 80, loss: 0.3659, val mf1: 0.7669, (best 0.7669)\n",
    "#### Epoch 81, loss: 0.3636, val mf1: 0.7663, (best 0.7669)\n",
    "#### Epoch 82, loss: 0.3603, val mf1: 0.7637, (best 0.7669)\n",
    "#### Epoch 83, loss: 0.3573, val mf1: 0.7661, (best 0.7669)\n",
    "#### Epoch 84, loss: 0.3547, val mf1: 0.7672, (best 0.7672)\n",
    "#### Epoch 85, loss: 0.3516, val mf1: 0.7686, (best 0.7686)\n",
    "#### Epoch 86, loss: 0.3498, val mf1: 0.7703, (best 0.7703)\n",
    "#### Epoch 87, loss: 0.3472, val mf1: 0.7728, (best 0.7728)\n",
    "#### Epoch 88, loss: 0.3439, val mf1: 0.7739, (best 0.7739)\n",
    "#### Epoch 89, loss: 0.3423, val mf1: 0.7743, (best 0.7743)\n",
    "#### Epoch 90, loss: 0.3395, val mf1: 0.7732, (best 0.7743)\n",
    "#### Epoch 91, loss: 0.3370, val mf1: 0.7735, (best 0.7743)\n",
    "#### Epoch 92, loss: 0.3352, val mf1: 0.7728, (best 0.7743)\n",
    "#### Epoch 93, loss: 0.3324, val mf1: 0.7755, (best 0.7755)\n",
    "#### Epoch 94, loss: 0.3305, val mf1: 0.7818, (best 0.7818)\n",
    "#### Epoch 95, loss: 0.3285, val mf1: 0.7812, (best 0.7818)\n",
    "#### Epoch 96, loss: 0.3267, val mf1: 0.7777, (best 0.7818)\n",
    "#### Epoch 97, loss: 0.3246, val mf1: 0.7791, (best 0.7818)\n",
    "#### Epoch 98, loss: 0.3231, val mf1: 0.7829, (best 0.7829)\n",
    "#### Epoch 99, loss: 0.3229, val mf1: 0.7807, (best 0.7829)\n",
    "#### time cost:  777.2296237945557 s\n",
    "#### Test: REC 63.30 PRE 61.09 MF1 77.81 AUC 91.33\n",
    "#### MF1-mean: 77.81, MF1-std: 0.00, AUC-mean: 91.33, AUC-std: 0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc61c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
