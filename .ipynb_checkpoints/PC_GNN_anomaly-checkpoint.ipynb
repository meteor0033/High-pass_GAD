{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7372750",
   "metadata": {},
   "source": [
    "# model source \n",
    "### Training PC-GNN\n",
    "### Paper: Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters\n",
    "### Source: https://github.com/PonderLY/PC-GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e81bf56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# 其中包括激活函数, 损失函数, 池化函数 ,通过 F.xxx() 的形式，可以方便地调用 torch.nn.functional 模块中的各种函数\n",
    "import numpy as np\n",
    "import numpy\n",
    "import argparse\n",
    "import time\n",
    "from dataset_process.dataset import Dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, roc_auc_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from layers.PC_GNN_layers.utils import pos_neg_split, normalize\n",
    "from model.PC_GNN_anomaly import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "184272c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, features, train_mask, val_mask, test_mask,labels, args):\n",
    "    \n",
    "    print('train/dev/test samples: ', train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_f1, final_tf1, final_trec, final_tpre, final_tmf1, final_tauc = 0., 0., 0., 0., 0., 0.\n",
    "\n",
    "    weight = (1-labels[train_mask]).sum().item() / labels[train_mask].sum().item()\n",
    "    print('cross entropy weight: ', weight)\n",
    "    time_start = time.time()\n",
    "    for e in range(args.epoch):\n",
    "        # 训练\n",
    "        model.train()\n",
    "        # 调用模型中的forward函数\n",
    "        logits = model(features)\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask], weight=torch.tensor([1., weight]))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #验证\n",
    "        model.eval()\n",
    "        probs = logits.softmax(1)\n",
    "        f1, thres = get_best_f1(labels[val_mask], probs[val_mask])\n",
    "        preds = np.zeros_like(labels)\n",
    "        preds[probs[:, 1] > thres] = 1\n",
    "        trec = recall_score(labels[test_mask], preds[test_mask])\n",
    "        tpre = precision_score(labels[test_mask], preds[test_mask])\n",
    "        tmf1 = f1_score(labels[test_mask], preds[test_mask], average='macro')\n",
    "        tauc = roc_auc_score(labels[test_mask], probs[test_mask][:, 1].detach().numpy())\n",
    "\n",
    "        if best_f1 < f1:\n",
    "            best_f1 = f1\n",
    "            final_trec = trec\n",
    "            final_tpre = tpre\n",
    "            final_tmf1 = tmf1\n",
    "            final_tauc = tauc\n",
    "        print('Epoch {}, loss: {:.4f}, val mf1: {:.4f}, (best {:.4f})'.format(e, loss, f1, best_f1))\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('time cost: ', time_end - time_start, 's')\n",
    "    print('Test: REC {:.2f} PRE {:.2f} MF1 {:.2f} AUC {:.2f}'.format(final_trec*100,\n",
    "                                                                     final_tpre*100, final_tmf1*100, final_tauc*100))\n",
    "    return final_tmf1, final_tauc\n",
    "\n",
    "\n",
    "# threshold adjusting for best macro f1\n",
    "def get_best_f1(labels, probs):\n",
    "    best_f1, best_thre = 0, 0\n",
    "    for thres in np.linspace(0.05, 0.95, 19):\n",
    "        #构建一个与labels同维度的数组,并初始化所有变量为零\n",
    "        preds = np.zeros_like(labels)\n",
    "        preds[probs[:,1] > thres] = 1\n",
    "        #average='binary'：计算二分类问题中的 F1 分数（默认值）。\n",
    "        #average='micro'：对所有类别的真实和预测样本进行汇总，然后计算 F1 分数。\n",
    "        #average='macro'：计算每个类别的 F1 分数，然后取平均值。\n",
    "        #average=None：返回每个类别的 F1 分数。\n",
    "        # F1_score 详细原理间“备份”\n",
    "        mf1 = f1_score(labels, preds, average='macro')\n",
    "        if mf1 > best_f1:\n",
    "            best_f1 = mf1\n",
    "            best_thre = thres\n",
    "    return best_f1, best_thre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28cdf3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='amazon', train_ratio=0.01, hid_dim=64, homo=1, epoch=100, run=1, lambda_1=2, no_cuda=False, emb_size=64, inter='GNN', step_size=0.02, model='CARE', batch_size=1024, rho=0.5, cuda=False)\n",
      "Done loading data from cached files.\n",
      "Graph(num_nodes=11944, num_edges=9569592,\n",
      "      ndata_schemes={'feature': Scheme(shape=(25,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'train_mask': Scheme(shape=(), dtype=torch.uint8), 'val_mask': Scheme(shape=(), dtype=torch.uint8), 'test_mask': Scheme(shape=(), dtype=torch.uint8), '_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)})\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PC_GNN_GAD')\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"yelp\",\n",
    "                        help=\"Dataset for this model (yelp/amazon/tfinance/tsocial)\")\n",
    "parser.add_argument(\"--train_ratio\", type=float, default=0.01, help=\"Training ratio\")\n",
    "parser.add_argument(\"--hid_dim\", type=int, default=64, help=\"Hidden layer dimension\")\n",
    "parser.add_argument(\"--homo\", type=int, default= 1, help=\"1 for PC_GNN_GAD(Homo) and 0 for PC_GNN_GAD(Hetero)\")\n",
    "parser.add_argument(\"--epoch\", type=int, default=100, help=\"The max number of epochs\")\n",
    "parser.add_argument(\"--run\", type=int, default=1, help=\"Running times\")\n",
    "parser.add_argument('--lambda_1', type=float, default=2, help='Simi loss weight.')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "parser.add_argument('--emb-size', type=int, default=64, help='Node embedding size at the last layer.')\n",
    "parser.add_argument('--inter', type=str, default='GNN', help='The inter-relation aggregator type. [Att, Weight, Mean, GNN]')\n",
    "parser.add_argument('--step-size', type=float, default=2e-2, help='RL action step size')\n",
    "parser.add_argument('--model', type=str, default='CARE', help='The model name. [CARE, SAGE]')\n",
    "parser.add_argument('--batch-size', type=int, default=1024, help='Batch size 1024 for yelp, 256 for amazon.')\n",
    "parser.add_argument('--rho', type=int, default=0.5, help='the ratio of the oversample neighbors for the minority class.')\n",
    "\n",
    "args = parser.parse_args(args = [])\n",
    "#args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "args.cuda = False\n",
    "\n",
    "print(args)\n",
    "dataset_name = args.dataset\n",
    "homo = args.homo\n",
    "h_feats = args.hid_dim\n",
    "graph = Dataset(dataset_name, homo).graph\n",
    "#edge_index = Dataset(dataset_name, homo).edge_index\n",
    "\n",
    "if (homo):\n",
    "    from layers.PC_GNN_layers.PC_GNN_layers_homo import *\n",
    "else:\n",
    "    from layers.PC_GNN_layers.PC_GNN_layers_hetero import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d6e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "############        从 DGL 图中获取图节点的邻居\n",
    "##################################################################################\n",
    "if (homo):\n",
    "    adj_lists = defaultdict(set)\n",
    "    # 获取每个节点的邻居节点并存储为 frozenset\n",
    "    for node in range(graph.num_nodes()):\n",
    "        neighbors = graph.successors(node)  # 对于出边邻居，使用 successors\n",
    "        for value in neighbors.tolist():\n",
    "            adj_lists[node].add(value)  \n",
    "else: ## only (dataset_name =='yelp') || (dataset_name =='yelp')\n",
    "    adj_list0 = defaultdict(set)\n",
    "    # 获取每个节点的邻居节点并存储为 frozenset\n",
    "    for node in range(graph[graph.canonical_etypes[0]].num_nodes()):\n",
    "        neighbors = graph[graph.canonical_etypes[0]].successors(node)  # 对于出边邻居，使用 successors\n",
    "        for value in neighbors.tolist():\n",
    "            adj_list0[node].add(value)\n",
    "                \n",
    "    adj_list1 = defaultdict(set)\n",
    "    # 获取每个节点的邻居节点并存储为 frozenset\n",
    "    for node in range(graph[graph.canonical_etypes[1]].num_nodes()):\n",
    "        neighbors = graph[graph.canonical_etypes[1]].successors(node)  # 对于出边邻居，使用 successors\n",
    "        for value in neighbors.tolist():\n",
    "            adj_list1[node].add(value)\n",
    "        \n",
    "    adj_list2 = defaultdict(set)\n",
    "    # 获取每个节点的邻居节点并存储为 frozenset\n",
    "    for node in range(graph[graph.canonical_etypes[2]].num_nodes()):\n",
    "        neighbors = graph[graph.canonical_etypes[2]].successors(node)  # 对于出边邻居，使用 successors\n",
    "        for value in neighbors.tolist():\n",
    "            adj_list2[node].add(value)\n",
    "    \n",
    "    adj_lists = [adj_list0, adj_list1, adj_list2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b5cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feats = graph.ndata['feature'].shape[1]\n",
    "features = graph.ndata['feature']\n",
    "features = normalize(features)\n",
    "\n",
    "labels = graph.ndata['label']\n",
    "index = list(range(len(labels)))\n",
    "if dataset_name == 'amazon':\n",
    "    index = list(range(3305, len(labels)))\n",
    "\n",
    "idx_train, idx_rest, y_train, y_rest = train_test_split(index, labels[index], stratify=labels[index],\n",
    "                                                            train_size=args.train_ratio,\n",
    "                                                            random_state=2, shuffle=True)\n",
    "idx_valid, idx_test, y_valid, y_test = train_test_split(idx_rest, y_rest, stratify=y_rest,\n",
    "                                                            test_size=0.67,\n",
    "                                                            random_state=2, shuffle=True)\n",
    "train_mask = torch.zeros([len(labels)]).bool()\n",
    "val_mask = torch.zeros([len(labels)]).bool()\n",
    "test_mask = torch.zeros([len(labels)]).bool()\n",
    "    \n",
    "\n",
    "train_mask[idx_train] = 1\n",
    "val_mask[idx_valid] = 1\n",
    "test_mask[idx_test] = 1\n",
    "\n",
    "\n",
    "train_pos, train_neg = pos_neg_split(idx_train, y_train)\n",
    "\n",
    "if (args.homo):\n",
    "    # build one-layer models\n",
    "    intra1 = IntraAgg(features, in_feats, args.emb_size,train_pos, rho = args.rho, cuda=args.cuda)\n",
    "    inter1 = InterAgg(features, in_feats, args.emb_size, train_pos, adj_lists, [intra1], inter=args.inter,cuda=args.cuda)\n",
    "else:\n",
    "    # build one-layer models\n",
    "    intra1 = IntraAgg(features, in_feats, args.emb_size,train_pos, rho = args.rho, cuda=args.cuda)\n",
    "    intra2 = IntraAgg(features, in_feats, args.emb_size,train_pos, rho = args.rho, cuda=args.cuda)\n",
    "    intra3 = IntraAgg(features, in_feats, args.emb_size,train_pos, rho = args.rho, cuda=args.cuda)\n",
    "    inter1 = InterAgg(features, in_feats, args.emb_size, train_pos, adj_lists, [intra1, intra2, intra3], inter=args.inter, cuda=args.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21660731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/dev/test samples:  86 2822 5731\n",
      "cross entropy weight:  9.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.6980, val mf1: 0.4751, (best 0.4751)\n",
      "Epoch 1, loss: 0.6728, val mf1: 0.6026, (best 0.6026)\n",
      "Epoch 2, loss: 0.6493, val mf1: 0.8920, (best 0.8920)\n",
      "Epoch 3, loss: 0.6219, val mf1: 0.9062, (best 0.9062)\n",
      "Epoch 4, loss: 0.5901, val mf1: 0.9075, (best 0.9075)\n",
      "Epoch 5, loss: 0.5539, val mf1: 0.9039, (best 0.9075)\n",
      "Epoch 6, loss: 0.5143, val mf1: 0.9045, (best 0.9075)\n",
      "Epoch 7, loss: 0.4730, val mf1: 0.9082, (best 0.9082)\n",
      "Epoch 8, loss: 0.4332, val mf1: 0.9116, (best 0.9116)\n",
      "Epoch 9, loss: 0.3979, val mf1: 0.9071, (best 0.9116)\n",
      "Epoch 10, loss: 0.3686, val mf1: 0.9050, (best 0.9116)\n",
      "Epoch 11, loss: 0.3477, val mf1: 0.9020, (best 0.9116)\n",
      "Epoch 12, loss: 0.3327, val mf1: 0.9069, (best 0.9116)\n",
      "Epoch 13, loss: 0.3237, val mf1: 0.9030, (best 0.9116)\n",
      "Epoch 14, loss: 0.3178, val mf1: 0.9088, (best 0.9116)\n",
      "Epoch 15, loss: 0.3131, val mf1: 0.9045, (best 0.9116)\n",
      "Epoch 16, loss: 0.3099, val mf1: 0.9058, (best 0.9116)\n",
      "Epoch 17, loss: 0.3074, val mf1: 0.9041, (best 0.9116)\n",
      "Epoch 18, loss: 0.3029, val mf1: 0.9004, (best 0.9116)\n",
      "Epoch 19, loss: 0.2980, val mf1: 0.8919, (best 0.9116)\n",
      "Epoch 20, loss: 0.2941, val mf1: 0.8798, (best 0.9116)\n",
      "Epoch 21, loss: 0.2897, val mf1: 0.8749, (best 0.9116)\n",
      "Epoch 22, loss: 0.2834, val mf1: 0.8739, (best 0.9116)\n",
      "Epoch 23, loss: 0.2783, val mf1: 0.8669, (best 0.9116)\n",
      "Epoch 24, loss: 0.2730, val mf1: 0.8642, (best 0.9116)\n",
      "Epoch 25, loss: 0.2664, val mf1: 0.8633, (best 0.9116)\n",
      "Epoch 26, loss: 0.2597, val mf1: 0.8653, (best 0.9116)\n",
      "Epoch 27, loss: 0.2532, val mf1: 0.8653, (best 0.9116)\n",
      "Epoch 28, loss: 0.2476, val mf1: 0.8642, (best 0.9116)\n",
      "Epoch 29, loss: 0.2417, val mf1: 0.8636, (best 0.9116)\n",
      "Epoch 30, loss: 0.2347, val mf1: 0.8636, (best 0.9116)\n",
      "Epoch 31, loss: 0.2278, val mf1: 0.8584, (best 0.9116)\n",
      "Epoch 32, loss: 0.2234, val mf1: 0.8563, (best 0.9116)\n",
      "Epoch 33, loss: 0.2190, val mf1: 0.8542, (best 0.9116)\n",
      "Epoch 34, loss: 0.2142, val mf1: 0.8481, (best 0.9116)\n",
      "Epoch 35, loss: 0.2084, val mf1: 0.8518, (best 0.9116)\n",
      "Epoch 36, loss: 0.2020, val mf1: 0.8482, (best 0.9116)\n",
      "Epoch 37, loss: 0.1972, val mf1: 0.8484, (best 0.9116)\n",
      "Epoch 38, loss: 0.1955, val mf1: 0.8512, (best 0.9116)\n",
      "Epoch 39, loss: 0.1913, val mf1: 0.8493, (best 0.9116)\n",
      "Epoch 40, loss: 0.1867, val mf1: 0.8494, (best 0.9116)\n",
      "Epoch 41, loss: 0.1836, val mf1: 0.8497, (best 0.9116)\n",
      "Epoch 42, loss: 0.1803, val mf1: 0.8491, (best 0.9116)\n",
      "Epoch 43, loss: 0.1756, val mf1: 0.8503, (best 0.9116)\n",
      "Epoch 44, loss: 0.1705, val mf1: 0.8506, (best 0.9116)\n",
      "Epoch 45, loss: 0.1668, val mf1: 0.8482, (best 0.9116)\n",
      "Epoch 46, loss: 0.1652, val mf1: 0.8503, (best 0.9116)\n",
      "Epoch 47, loss: 0.1630, val mf1: 0.8479, (best 0.9116)\n",
      "Epoch 48, loss: 0.1594, val mf1: 0.8506, (best 0.9116)\n",
      "Epoch 49, loss: 0.1535, val mf1: 0.8512, (best 0.9116)\n",
      "Epoch 50, loss: 0.1481, val mf1: 0.8527, (best 0.9116)\n",
      "Epoch 51, loss: 0.1443, val mf1: 0.8542, (best 0.9116)\n",
      "Epoch 52, loss: 0.1425, val mf1: 0.8512, (best 0.9116)\n",
      "Epoch 53, loss: 0.1396, val mf1: 0.8547, (best 0.9116)\n",
      "Epoch 54, loss: 0.1327, val mf1: 0.8535, (best 0.9116)\n",
      "Epoch 55, loss: 0.1281, val mf1: 0.8558, (best 0.9116)\n",
      "Epoch 56, loss: 0.1266, val mf1: 0.8604, (best 0.9116)\n",
      "Epoch 57, loss: 0.1284, val mf1: 0.8579, (best 0.9116)\n",
      "Epoch 58, loss: 0.1297, val mf1: 0.8654, (best 0.9116)\n",
      "Epoch 59, loss: 0.1202, val mf1: 0.8604, (best 0.9116)\n",
      "Epoch 60, loss: 0.1128, val mf1: 0.8586, (best 0.9116)\n",
      "Epoch 61, loss: 0.1154, val mf1: 0.8581, (best 0.9116)\n",
      "Epoch 62, loss: 0.1127, val mf1: 0.8538, (best 0.9116)\n",
      "Epoch 63, loss: 0.1045, val mf1: 0.8536, (best 0.9116)\n",
      "Epoch 64, loss: 0.1036, val mf1: 0.8550, (best 0.9116)\n",
      "Epoch 65, loss: 0.1058, val mf1: 0.8521, (best 0.9116)\n",
      "Epoch 66, loss: 0.0981, val mf1: 0.8578, (best 0.9116)\n",
      "Epoch 67, loss: 0.0931, val mf1: 0.8587, (best 0.9116)\n",
      "Epoch 68, loss: 0.0940, val mf1: 0.8598, (best 0.9116)\n",
      "Epoch 69, loss: 0.0907, val mf1: 0.8649, (best 0.9116)\n",
      "Epoch 70, loss: 0.0859, val mf1: 0.8655, (best 0.9116)\n",
      "Epoch 71, loss: 0.0861, val mf1: 0.8607, (best 0.9116)\n",
      "Epoch 72, loss: 0.0844, val mf1: 0.8638, (best 0.9116)\n",
      "Epoch 73, loss: 0.0792, val mf1: 0.8597, (best 0.9116)\n",
      "Epoch 74, loss: 0.0787, val mf1: 0.8603, (best 0.9116)\n",
      "Epoch 75, loss: 0.0784, val mf1: 0.8624, (best 0.9116)\n",
      "Epoch 76, loss: 0.0741, val mf1: 0.8633, (best 0.9116)\n",
      "Epoch 77, loss: 0.0708, val mf1: 0.8624, (best 0.9116)\n",
      "Epoch 78, loss: 0.0714, val mf1: 0.8624, (best 0.9116)\n",
      "Epoch 79, loss: 0.0701, val mf1: 0.8614, (best 0.9116)\n",
      "Epoch 80, loss: 0.0661, val mf1: 0.8618, (best 0.9116)\n",
      "Epoch 81, loss: 0.0643, val mf1: 0.8624, (best 0.9116)\n",
      "Epoch 82, loss: 0.0644, val mf1: 0.8603, (best 0.9116)\n",
      "Epoch 83, loss: 0.0627, val mf1: 0.8612, (best 0.9116)\n",
      "Epoch 84, loss: 0.0598, val mf1: 0.8622, (best 0.9116)\n",
      "Epoch 85, loss: 0.0585, val mf1: 0.8625, (best 0.9116)\n",
      "Epoch 86, loss: 0.0577, val mf1: 0.8618, (best 0.9116)\n",
      "Epoch 87, loss: 0.0559, val mf1: 0.8616, (best 0.9116)\n",
      "Epoch 88, loss: 0.0545, val mf1: 0.8622, (best 0.9116)\n",
      "Epoch 89, loss: 0.0535, val mf1: 0.8612, (best 0.9116)\n",
      "Epoch 90, loss: 0.0523, val mf1: 0.8627, (best 0.9116)\n",
      "Epoch 91, loss: 0.0515, val mf1: 0.8609, (best 0.9116)\n",
      "Epoch 92, loss: 0.0504, val mf1: 0.8594, (best 0.9116)\n",
      "Epoch 93, loss: 0.0489, val mf1: 0.8599, (best 0.9116)\n",
      "Epoch 94, loss: 0.0476, val mf1: 0.8572, (best 0.9116)\n",
      "Epoch 95, loss: 0.0471, val mf1: 0.8576, (best 0.9116)\n",
      "Epoch 96, loss: 0.0471, val mf1: 0.8582, (best 0.9116)\n",
      "Epoch 97, loss: 0.0472, val mf1: 0.8558, (best 0.9116)\n",
      "Epoch 98, loss: 0.0456, val mf1: 0.8576, (best 0.9116)\n",
      "Epoch 99, loss: 0.0430, val mf1: 0.8576, (best 0.9116)\n",
      "time cost:  1021.1301655769348 s\n",
      "Test: REC 75.96 PRE 89.22 MF1 90.16 AUC 90.82\n",
      "MF1-mean: 90.16, MF1-std: 0.00, AUC-mean: 90.82, AUC-std: 0.00\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "if args.run == 0:\n",
    "    \n",
    "    model = PCGNN_GAD(in_feats, h_feats, num_classes, graph, args.lambda_1)\n",
    "    train(model, features, train_mask, val_mask, test_mask, labels, args)\n",
    "\n",
    "else:\n",
    "    \n",
    "    final_mf1s, final_aucs = [], []\n",
    "    for tt in range(args.run):\n",
    "        #in_feats 特征点维度；h_feats：隐层维度；num_classes：节点分类数（nomal，anomaly）\n",
    "        model = PCGNN_GAD(in_feats, h_feats, num_classes, graph,inter1,args.lambda_1)\n",
    "        \n",
    "        mf1, auc = train(model, features, train_mask, val_mask, test_mask,labels, args)\n",
    "        final_mf1s.append(mf1)\n",
    "        final_aucs.append(auc)\n",
    "    final_mf1s = np.array(final_mf1s)\n",
    "    final_aucs = np.array(final_aucs)\n",
    "    # np.std :计算全局标准差\n",
    "    print('MF1-mean: {:.2f}, MF1-std: {:.2f}, AUC-mean: {:.2f}, AUC-std: {:.2f}'.format(100 * np.mean(final_mf1s),\n",
    "                                                                                            100 * np.std(final_mf1s),\n",
    "                                                               100 * np.mean(final_aucs), 100 * np.std(final_aucs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633a9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80106dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c3fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76331b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3fe09b-6bab-42b9-8f88-f9b0f93b2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### test #############################\n",
    "#######################################\n",
    "features = graph.ndata['feature']\n",
    "nodes = graph.nodes()\n",
    "thresholds = [0.5, 0.5, 0.5]\n",
    "\n",
    "to_neighs = []\n",
    "for adj_list in adj_lists:\n",
    "\tto_neighs.append([set(adj_list[node.item()]) for node in nodes])\n",
    "# find unique nodes and their neighbors used in current batch\n",
    "unique_nodes = set.union(set.union(*to_neighs[0]), set.union(*to_neighs[1]),\n",
    "                            set.union(*to_neighs[2], set(nodes.numpy())))\n",
    "unique_nodes_list = list(unique_nodes)\n",
    "\n",
    "\t\t\n",
    "# calculate label-aware scores\n",
    "if False:\n",
    "\tbatch_features = features[torch.cuda.LongTensor(list(unique_nodes))]\n",
    "else:\n",
    "\tbatch_features = features[torch.tensor(unique_nodes_list, dtype=torch.long)]\n",
    "label_clf = nn.Linear(features.shape[1], 2)\n",
    "batch_scores = label_clf(batch_features)\n",
    "# 创建节点ID到索引的映射\n",
    "id_mapping = {node_id: index for index, node_id in enumerate(unique_nodes_list)}\n",
    "center_nodes = []\n",
    "for node in unique_nodes:\n",
    "\tcenter_nodes.append(id_mapping[node])\n",
    "center_scores = batch_scores[torch.tensor(center_nodes, dtype=torch.long), :]\n",
    "\n",
    "# get neighbor node id list for each batch node and relation\n",
    "r1_list = [list(to_neigh) for to_neigh in to_neighs[0]]\n",
    "r2_list = [list(to_neigh) for to_neigh in to_neighs[1]]\n",
    "r3_list = [list(to_neigh) for to_neigh in to_neighs[2]]\n",
    "\n",
    "# assign label-aware scores to neighbor nodes for each batch node and relation\n",
    "r1_scores = []\n",
    "for to_neigh in r1_list:\n",
    "\tif len(to_neigh) > 0:\n",
    "\t\tindices = list(itemgetter(*to_neigh)(id_mapping)) if len(to_neigh) > 1 else [id_mapping[next(iter(to_neigh))]]\n",
    "\t\tr1_scores.append(batch_scores[torch.tensor(indices, dtype=torch.long), :].view(-1, 2))\n",
    "\telse:\n",
    "\t\tr1_scores.append(torch.empty(0, 2))  # 如果为空，添加一个空的 tensor\n",
    "        \n",
    "r2_scores = []\n",
    "for to_neigh in r2_list:\n",
    "\tif len(to_neigh) > 0:\n",
    "\t\tindices = list(itemgetter(*to_neigh)(id_mapping)) if len(to_neigh) > 1 else [id_mapping[next(iter(to_neigh))]]\n",
    "\t\tr2_scores.append(batch_scores[torch.tensor(indices, dtype=torch.long), :].view(-1, 2))\n",
    "\telse:\n",
    "\t\tr2_scores.append(torch.empty(0, 2))  # 如果为空，添加一个空的 tensor\n",
    "                \n",
    "r3_scores = []\n",
    "for to_neigh in r3_list:\n",
    "\tif len(to_neigh) > 0:\n",
    "\t\tindices = list(itemgetter(*to_neigh)(id_mapping)) if len(to_neigh) > 1 else [id_mapping[next(iter(to_neigh))]]\n",
    "\t\tr3_scores.append(batch_scores[torch.tensor(indices, dtype=torch.long), :].view(-1, 2))\n",
    "\telse:\n",
    "\t\tr3_scores.append(torch.empty(0, 2))  # 如果为空，添加一个空的 tensor\n",
    "\n",
    "# count the number of neighbors kept for aggregation for each batch node and relation\n",
    "r1_sample_num_list = [math.ceil(len(neighs) * thresholds[0]) for neighs in r1_list]\n",
    "r2_sample_num_list = [math.ceil(len(neighs) * thresholds[1]) for neighs in r2_list]\n",
    "r3_sample_num_list = [math.ceil(len(neighs) * thresholds[2]) for neighs in r3_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa725b6-4a65-4480-bc6f-2f5dfaa603a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_neighs_ada_threshold(center_scores, neigh_scores, neighs_list, sample_list):\n",
    "\t\"\"\"\n",
    "\tFilter neighbors according label predictor result with adaptive thresholds\n",
    "\t:param center_scores: the label-aware scores of batch nodes\n",
    "\t:param neigh_scores: the label-aware scores 1-hop neighbors each batch node in one relation\n",
    "\t:param neighs_list: neighbor node id list for each batch node in one relation\n",
    "\t:param sample_list: the number of neighbors kept for each batch node in one relation\n",
    "\t:return samp_neighs: the neighbor indices and neighbor simi scores\n",
    "\t:return samp_scores: the average neighbor distances for each relation after filtering\n",
    "\t\"\"\"\n",
    "\n",
    "\tsamp_neighs = []\n",
    "\tsamp_scores = []\n",
    "\tfor idx, center_score in enumerate(center_scores):\n",
    "\t\tcenter_score = center_scores[idx][0]\n",
    "\t\tneigh_score = neigh_scores[idx][:, 0].view(-1, 1)\n",
    "\t\tcenter_score = center_score.repeat(neigh_score.size()[0], 1)\n",
    "\t\tneighs_indices = neighs_list[idx]\n",
    "\t\tnum_sample = sample_list[idx]\n",
    "\n",
    "\t\t# compute the L1-distance of batch nodes and their neighbors\n",
    "\t\t# Eq. (2) in paper\n",
    "\t\tscore_diff = torch.abs(center_score - neigh_score).squeeze()\n",
    "\t\tsorted_scores, sorted_indices = torch.sort(score_diff, dim=0, descending=False)\n",
    "\t\tselected_indices = sorted_indices.tolist()\n",
    "\n",
    "\t\t# top-p sampling according to distance ranking and thresholds\n",
    "\t\t# Section 3.3.1 in paper\n",
    "\t\tif len(neigh_scores[idx]) > num_sample + 1:\n",
    "\t\t\tselected_neighs = [neighs_indices[n] for n in selected_indices[:num_sample]]\n",
    "\t\t\tselected_scores = sorted_scores.tolist()[:num_sample]\n",
    "\t\telse:\n",
    "\t\t\tselected_neighs = neighs_indices\n",
    "\t\t\tselected_scores = score_diff.tolist()\n",
    "\t\t\tif isinstance(selected_scores, float):\n",
    "\t\t\t\tselected_scores = [selected_scores]\n",
    "\n",
    "\t\tsamp_neighs.append(set(selected_neighs))\n",
    "\t\tsamp_scores.append(selected_scores)\n",
    "\n",
    "\treturn samp_neighs, samp_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc4672a-c010-4691-a49f-2a6627cfc787",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca7e29-cccc-44ad-afd4-a678b2678848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward(self, nodes, to_neighs_list, batch_scores, neigh_scores, sample_list)\n",
    "to_neighs_list = r2_list\n",
    "batch_scores = center_scores\n",
    "neigh_scores = r2_scores\n",
    "sample_list = r2_sample_num_list\n",
    "samp_neighs, samp_scores = filter_neighs_ada_threshold(batch_scores, neigh_scores, to_neighs_list, sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e680a3e-7e10-4d39-a2f2-91abfa85bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nodes_list = list(set.union(*samp_neighs))\n",
    "unique_nodes = {n: i for i, n in enumerate(unique_nodes_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b9fc3-753c-45df-918c-a4daea8fc6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n",
    "column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
    "row_indices = [i for i in range(len(samp_neighs)) for _ in range(len(samp_neighs[i]))]\n",
    "mask[row_indices, column_indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b6f9e-5663-4e55-a117-8f45c8a32e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neigh = mask.sum(1, keepdim=True)\n",
    "num_neigh[num_neigh == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce6d28-86bc-463a-a955-30e4fbcb4c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b568758-bc22-4a6a-8e12-00e451c22722",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask.div(num_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd19e1-4cbc-4486-ad91-f8c50a73a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sum(sum(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd4bbf-a2df-4446-9bd7-e64d8c1c2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
    "row_indices = [i for i in range(len(samp_neighs)) for _ in range(len(samp_neighs[i]))]\n",
    "mask[row_indices, column_indices] = 1\n",
    "if False:\n",
    "\tmask = mask.cuda()\n",
    "num_neigh = mask.sum(1, keepdim=True)\n",
    "mask = mask.div(num_neigh)\n",
    "if False:\n",
    "\tembed_matrix = features[torch.LongTensor(unique_nodes_list).cuda()]\n",
    "else:\n",
    "\tembed_matrix = features[torch.LongTensor(unique_nodes_list)]\n",
    "to_feats = mask.mm(embed_matrix)\n",
    "to_feats = F.relu(to_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba2e6c1-5b27-4f18-a6d4-a2a810c5a32b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7b61665",
   "metadata": {},
   "source": [
    "##  Test\n",
    "### 1. 按种类获取边（边存放在元组中，需要将元组转化为Tensor， 需要按边的种类，分别转化，不能一次将存放在元组中的边，转化为teosor）\n",
    "### 2. ChebConv 模型需要边的输入类型为 二维Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import FraudYelpDataset, FraudAmazonDataset\n",
    "dataset = FraudAmazonDataset()\n",
    "graph = dataset[0]\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d131ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dgl.to_homogeneous(dataset[0], ndata=['feature', 'label', 'train_mask', 'val_mask', 'test_mask'])\n",
    "graph = dgl.add_self_loop(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5451c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.local_scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import FraudYelpDataset, FraudAmazonDataset\n",
    "dataset = FraudYelpDataset()\n",
    "graph = dataset[0]\n",
    "graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c6121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for relation in graph.canonical_etypes:\n",
    "    print(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_uvu = graph[relation].edges()\n",
    "edge_index = torch.stack(edges_uvu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#三类边\n",
    "edges_upu = graph[graph.canonical_etypes[0]].edges()\n",
    "edge_index_upu = torch.stack(edges_upu)\n",
    "edges_usu = graph[graph.canonical_etypes[1]].edges()\n",
    "edge_index_usu = torch.stack(edges_usu)\n",
    "edges_uvu = graph[graph.canonical_etypes[2]].edges()\n",
    "edge_index_uvu = torch.stack(edges_uvu)\n",
    "\n",
    "\n",
    "# 合并连个Tensor，dim=1 按列合并\n",
    "combined_tensor = torch.cat((edge_index_upu, edge_index_usu), dim=1)\n",
    "edge_index = torch.cat((combined_tensor, edge_index_uvu), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c1a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"edge_index_upu.shape:\",edge_index_upu.shape)\n",
    "print(\"edge_index_usu.shape:\",edge_index_usu.shape)\n",
    "print(\"edge_index_uvu.shape:\",edge_index_uvu.shape)\n",
    "print(\"edge_index.shape:\",edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ad58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = graph[graph.canonical_etypes[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b3c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import FraudYelpDataset, FraudAmazonDataset\n",
    "dataset = FraudAmazonDataset()\n",
    "graph = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_upu = graph[graph.canonical_etypes[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7849077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_upu.ndata['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2452df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if homo:\n",
    "    graph = dgl.to_homogeneous(dataset[0], ndata=['feature', 'label', 'train_mask', 'val_mask', 'test_mask'])\n",
    "  \n",
    "    graph = dgl.add_self_loop(graph)\n",
    "                \n",
    "    #三类边\n",
    "    edges_upu = graph[graph.canonical_etypes[0]].edges()\n",
    "    edge_index_upu = torch.stack(edges_upu)\n",
    "    edges_usu = graph[graph.canonical_etypes[1]].edges()\n",
    "    edge_index_usu = torch.stack(edges_usu)\n",
    "    edges_uvu = graph[graph.canonical_etypes[2]].edges()\n",
    "    edge_index_uvu = torch.stack(edges_uvu)\n",
    "                \n",
    "    # 合并连个Tensor，dim=1 按列合并\n",
    "    combined_tensor = torch.cat((edge_index_upu, edge_index_usu), dim=1)\n",
    "    edge_index = torch.cat((combined_tensor, edge_index_uvu), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e7d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.canonical_etypes)\n",
    "edge_index = graph.edges()\n",
    "edge_index = torch.stack(edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92770cce",
   "metadata": {},
   "source": [
    "### 激活函数方面：nn.LeakyReLU(): LeakyReLU 是 ReLU 的一个变种，它允许负值通过一个小的斜率而不是将它们直接设为零。这个斜率是一个超参数，通常设置为一个小的正数，比如 0.01。这样做的目的是解决“ReLU 死亡神经元”问题，即某些神经元在训练过程中可能永远不会被激活，从而停止更新权重。LeakyReLU 在这种情况下可以提供更好的梯度流动，帮助网络更快地收敛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d185109-a78a-4283-8822-cdda3994e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义几个集合\n",
    "set1 = {1, 2, 3}\n",
    "set2 = {3, 1, 2}\n",
    "set3 = {5, 3, 4}\n",
    "\n",
    "# 使用 set.union() 方法\n",
    "result = set1.union(set2, set3)\n",
    "\n",
    "print(result)  # 输出: {1, 2, 3, 4, 5, 6, 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c37cb-e9e5-4869-9947-a50406e27a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
